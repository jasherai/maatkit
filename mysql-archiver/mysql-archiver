#!/usr/bin/perl

# This is mysql-archiver, a program to archive records from one MySQL table to
# a file and/or another table.
#
# This program is copyright (c) 2007 Baron Schwartz, baron at xaprb dot com.
# Feedback and improvements are welcome.
#
# THIS PROGRAM IS PROVIDED "AS IS" AND WITHOUT ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF
# MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.
#
# This program is free software; you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, version 2; OR the Perl Artistic License.  On UNIX and similar
# systems, you can issue `man perlgpl' or `man perlartistic' to read these
# licenses.
#
# You should have received a copy of the GNU General Public License along with
# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
# Place, Suite 330, Boston, MA  02111-1307  USA.

use strict;
use warnings FATAL => 'all';

use DBI;
use English qw(-no_match_vars);
use Getopt::Long;
use List::Util qw(max);
use IO::File;

# Output to a file or to STDOUT
# Specify where to archive to (file, other DB)
# specify table, index to use
# specify earliest date range
# specify how many in a chunk, whether to use txn
# specify maximum runtime
# Allow to ANALYZE TABLE or OPTIMIZE TABLE after or before
# whether to print progress to output, and how often (how many rows, how many
# secs?)
# Use HANDLER interface: http://dev.mysql.com/doc/refman/5.0/en/handler.html

our $VERSION = '@VERSION@';

# ############################################################################
# Get configuration information.
# ############################################################################

# Define cmdline args.
my @opt_spec = (
   { s => 'buffer|b',    d => 'Buffer output to --file and flush at commit' },
   { s => 'dest|d=s',    d => 'Table to archive to' },
   { s => 'file|f=s',    d => 'File to archive to, with DATE_FORMAT()-like formatting' },
   { s => 'header|h',    d => 'Print column header at top of --file' },
   { s => 'help',        d => 'Show this help message' },
   { s => 'ignore|i',    d => 'Use IGNORE for INSERT statements' },
   { s => 'limit|l=i',   d => 'Number of rows to fetch and archive per statement' },
   { s => 'progress|P=i',d => 'Print progress information every X rows' },
   { s => 'purge|p',     d => 'Purge instead of archive; allows to omit --file and --dest' },
   { s => 'replace|r',   d => 'Use REPLACE instead of INSERT statements' },
   { s => 'sentinel=s',  d => 'Sentinel file; default /tmp/mysql-archiver-sentinel' },
   { s => 'sleep|e=i',   d => 'Sleep time between fetches' },
   { s => 'source|s=s',  d => 'Table to archive from' },
   { s => 'test|t',      d => 'Test: print queries and exit without doing anything' },
   { s => 'time|m=s',    d => 'Time to run before exiting (suffix: s/m/h/d)' },
   { s => 'txnsize|z=i', d => 'Transaction size in number of rows (default 1; 0 disables)' },
   { s => 'where|W=s',   d => 'WHERE clause to limit which rows to archive' },
);

# This is the container for the command-line options' values to be stored in
# after processing.  Initial values are defaults.
my %opts = (
   l        => 1,
   u        => 1,
   z        => 1,
   sentinel => '/tmp/mysql-archiver-sentinel',
);

# Post-process...
my %opt_seen;
foreach my $spec ( @opt_spec ) {
   my ( $long, $short ) = $spec->{s} =~ m/^([\w-]+)(?:\|([^!+=]*))?/;
   $spec->{k} = $short || $long;
   $spec->{l} = $long;
   $spec->{t} = $short;
   $spec->{n} = $spec->{s} =~ m/!/;
   $opts{$spec->{k}} = undef unless defined $opts{$spec->{k}};
   die "Duplicate option $spec->{k}" if $opt_seen{$spec->{k}}++;
}

Getopt::Long::Configure('no_ignore_case', 'bundling');
GetOptions( map { $_->{s} => \$opts{$_->{k}} } @opt_spec) or $opts{help} = 1;

my %conn   = (
   h => 'host',
   P => 'port',
   S => 'socket',
   u => 'user',
   p => 'pass',
   F => 'mysql_read_default_file',
   D => 'database',
   t => 'table',
   i => 'index',
);

my $src = parse_dsn($opts{s});
my $dst = parse_dsn($opts{d}, $src) if $src && $opts{d};

# Generate a filename with sprintf-like formatting codes.
if ( $opts{f} ) {
   my @time = localtime();
   my %fmt = (
      d => sprintf('%02d', $time[3]),
      H => sprintf('%02d', $time[2]),
      i => sprintf('%02d', $time[1]),
      m => sprintf('%02d', $time[4] + 1),
      s => sprintf('%02d', $time[0]),
      Y => $time[5] + 1900,
      D => $src && $src->{D} ? $src->{D} : '',
      t => $src && $src->{t} ? $src->{t} : '',
   );
   $opts{f} =~ s/%([dHimsYDt])/$fmt{$1}/g;
}

if ( !$opts{help} ) {
   if ( $opts{m} ) {
      if ( $opts{m} !~ m/^\d+[smhd]$/ ) {
         warn "Invalid --time argument\n";
         $opts{help} = 1;
      }
      elsif ( $opts{m} =~ m/([smhd])$/ ) {
         $opts{m} *= $1 eq 's' ? 1        # Seconds
                   : $1 eq 'm' ? 60       # Minutes
                   : $1 eq 'h' ? 3600     # Hours
                   :             86400;   # Days
      }
   }
   elsif ( !$src || !$src->{t} ) {
      warn "Invalid or missing --source argument\n";
      $opts{help} = 1;
   }
   elsif ( !($dst || $opts{f}) && !$opts{p} ) {
      warn "No --dest or --file; this would delete rows (override with --purge)\n";
      $opts{help} = 1;
   }
   elsif ( $opts{d} ) { # Ensure --source and --dest don't point to the same place
      my $same = 1;
      foreach my $arg ( qw(h P D t S) ) {
         if ( defined $src->{$arg} && defined $dst->{$arg} &&
            $src->{$arg} ne $dst->{$arg} ) {
            $same = 0;
            last;
         }
      }
      if ( $same ) {
         warn "--source and --dest refer to the same table\n";
         $opts{help} = 1;
      }
   }
}

if ( $opts{help} ) {
   print "Usage: mysql-archiver <options>\n\n";
   my $maxw = max(map { length($_->{l}) + ($_->{n} ? 4 : 0)} @opt_spec);
   foreach my $spec ( sort { $a->{l} cmp $b->{l} } @opt_spec ) {
      my $long  = $spec->{n} ? "[no]$spec->{l}" : $spec->{l};
      my $short = $spec->{t} ? "-$spec->{t}" : '';
      printf("  --%-${maxw}s %-4s %s\n", $long, $short, $spec->{d});
   }
   print <<USAGE;

mysql-archiver nibbles records from a single table.  Connection options are
read from MySQL option files.  The --source and --dest arguments require a
special format.  For more details, please read the documentation:

   perldoc mysql-archiver

USAGE
   exit(1);
}

# ############################################################################
# Get ready to do the main work.
# ############################################################################
my $can_retry = 1;
foreach my $table ( grep { $_ } ($src, $dst) ) {
   my $dbh        = get_dbh($table);
   $table->{dbh}  = $dbh;
   $table->{irot} = get_irot($dbh);

   $can_retry = $can_retry && $table->{irot};

   $table->{db_tbl} =
      join('.',
      map  { $dbh->quote_identifier($_) }
      grep { $_ }
      map  { $_ =~ s/(^`|`$)//g; $_; }
      grep { $_ }
      ( $table->{D}, $table->{t} ));
   $table->{info} = get_tbl_struct($table);
}

# TODO: catch signals and exit gracefully (INT, TERM)
# TODO: allow to specify ODKU option?
# TODO: allow multiple destination tables, each with own ODKU/etc
# TODO: wrap all in eval, retry deadlocks (find out when behavior changed)
# TODO: analyze tables


my $dbh = $src->{dbh};
my $sth;

# ############################################################################
# Design SQL statements.
# ############################################################################
my ($first_sql, $next_sql, $del_sql, $ins_sql);
my (@pk_slice, @asc_slice);
my @cols = @{$src->{info}->{cols}};

# Do we have an index to ascend?  Use PRIMARY if nothing specified.
if ( $src->{i} || $src->{info}->{keys}->{PRIMARY} ) {
   # Make sure the lettercase is right and find the index...
   my $ixname = $src->{i} || '';
   if ( uc $ixname eq 'PRIMARY' || !$src->{i} ) {
      $ixname = 'PRIMARY';
   }
   else {
      ($ixname) = grep { uc $_ eq uc $src->{i} } keys %{$src->{info}->{keys}};
   }

   if ( $ixname ) {
      $src->{i} = $ixname; # Corrects lettercase if it's wrong
   }
   my @asc_cols = @{$src->{info}->{keys}->{$ixname}};

   # We found the columns, now find their positions
   if ( @asc_cols ) {
      @asc_slice = map {
         my $pos = 0;
         my $col = $_;
         foreach my $c ( @cols ) {
            last if $c eq $col;
            ++$pos;
         }
         $pos;
      } @asc_cols;
   }
}

$first_sql
   = 'SELECT'
   . ( version_ge($dbh, '4.0.1') ? ' SQL_NO_CACHE' : '' )
   . " * FROM $src->{db_tbl}"
   . ( $src->{i}
      ? ((version_ge($dbh, '4.0.9') ? " FORCE" : " USE") . " INDEX(`$src->{i}`)")
      : '');
if ( $opts{W} ) {
   $first_sql .= " WHERE $opts{W}";
}

# At this point the fetch-first and fetch-next queries may diverge.
$next_sql = $first_sql;
if ( @asc_slice ) {
   $next_sql .= $opts{W} ? ' AND ' : ' WHERE ';
   $next_sql .= join(' AND ', map { "`$cols[$_]` >= ?" } @asc_slice);
}

$first_sql .= " LIMIT $opts{l}";
$next_sql  .= " LIMIT $opts{l}";

# DELETE requires either a PK or all columns.  In theory, a UNIQUE index could
# be used, but I am not going to fool with that.
if ( $src->{info}->{keys}->{PRIMARY} ) {
   @pk_slice = map {
      my $pos = 0;
      my $col = $_;
      foreach my $c ( @cols ) {
         last if $c eq $col;
         ++$pos;
      }
      $pos;
   } @{$src->{info}->{keys}->{PRIMARY}};
}
else {
   @pk_slice = (0 .. $#cols);
}

# The LIMIT is *always* 1 here, because even though a SELECT can return many
# rows, an INSERT only does one at a time.  It would not be safe to iterate
# over a SELECT that was LIMIT-ed to 500 rows, read and INSERT one, and then
# delete with a LIMIT of 500.  Only one row would be written to the file; only
# one would be INSERT-ed at the destination.  Every DELETE must be LIMIT 1.
$del_sql = "DELETE FROM $src->{db_tbl} WHERE "
   . join(' AND ', map { "`$cols[$_]` = ?" } @pk_slice)
   . " LIMIT 1";

# INSERT is all columns.  I can't think of why you'd want to archive to a
# table with different columns than the source.
if ( $dst ) {
   $ins_sql = $opts{r} ? 'REPLACE'
            : $opts{i} ? 'INSERT IGNORE'
            :            'INSERT';
   $ins_sql .= " INTO $dst->{db_tbl}("
      . join(",", map { "`$_`" } @cols)
      . ") VALUES ("
      . join(",", map { "?" } @cols)
      . ")";
}
else {
   $ins_sql = '';
}

if ( $opts{t} ) {
   print join("\n", ($opts{f} || ''), $first_sql, $next_sql, $del_sql, $ins_sql), "\n";
   exit(0);
}

my $get_first = $dbh->prepare($first_sql);
my $get_next  = $dbh->prepare($next_sql);
my $del_row   = $dbh->prepare($del_sql);
my $ins_row   = $dst->{dbh}->prepare($ins_sql) if $dst; # Different $dbh!

# ############################################################################
# Start archiving.
# ############################################################################
my $start   = time();
my $end     = $start + ($opts{m} || 0); # When mysql-archiver should exit
my $now     = $start;
my $cnt     = 0;
my $txn_cnt = 0;
printf("%-19s %7s %7s\n", 'TIME', 'ELAPSED', 'COUNT') if $opts{P};
printf("%19s %7d %7d\n", ts($now), $now - $start, $cnt) if $opts{P};

my $get_sth = $get_first; # Later it may be assigned $get_next
$get_sth->execute;
my $row = $get_sth->fetchrow_arrayref();
if ( !$row ) {
   $get_sth->finish;
   $src->{dbh}->disconnect();
   $dst->{dbh}->disconnect() if $dst;
   exit(0);
}

# Open the file and print the header to it.
my $file;
if ( $opts{f} ) {
   my $need_hdr = $opts{h} && !-f $opts{f};
   $file = IO::File->new($opts{f}, ">>")
      or die "Cannot open $opts{f}: $OS_ERROR\n";
   $file->autoflush(1) unless $opts{b};
   if ( $need_hdr ) {
      print $file escape(\@cols), "\n"
         or die "Cannot write to $opts{f}: $OS_ERROR\n";
   }
}

# Holds the arguments for the $sth's bind variables, so it can be re-tried
# easily.
my @beginning_of_txn;

ROW:
while ( $row && (!$opts{m} || $now < $end) && !-f $opts{sentinel} ) {
   my $lastrow = $row;

   # Do the archiving.  Write to the file first, since like the file, MyISAM
   # and other tables cannot be rolled back etc.  If there is a problem,
   # hopefully the data has at least made it to the file.
   if ( $file ) {
      print $file escape($row), "\n"
         or die "Cannot write to $opts{f}: $OS_ERROR\n";
   }

   if ( $dst ) {
      # TODO: only retry a finite number of times.
      next ROW unless do_with_retries( sub { $ins_row->execute(@$row) } );
   }

   # DELETE must come after INSERT, to be as safe as possible.
   next ROW unless do_with_retries( sub { $del_row->execute(@{$row}[@pk_slice]) } );

   $now = time();
   ++$cnt;
   ++$txn_cnt;

   # Possibly flush the file and commit the insert and delete.
   if ( $opts{z} && $cnt % $opts{z} == 0 ) {
      if ( $opts{b} && $file ) {
         $file->flush or die "Cannot flush $opts{f}: $OS_ERROR\n";
      }
      $dst->{dbh}->commit if $dst;
      $src->{dbh}->commit;
      $txn_cnt = 0;
   }

   # Report on progress.
   if ( $opts{P} && $cnt % $opts{P} == 0 ) {
      printf("%19s %7d %7d\n", ts($now), $now - $start, $cnt);
   }

   # Get the next row.
   if ( $get_sth->{Active} ) { # Fetch until exhausted
      $row = $get_sth->fetchrow_arrayref();
   }
   if ( !$row ) {
      sleep($opts{e})   if $opts{e};
      $get_sth          = $get_next;
      $get_next->execute(@{$lastrow}[@asc_slice]);
      @beginning_of_txn = @{$lastrow}[@asc_slice] unless $txn_cnt;
      $row              = $get_sth->fetchrow_arrayref();
   }
}

# Transactions might still be open, etc
if ( $opts{f} && $file ) {
   close $file
      or die "Cannot close $opts{f}: $OS_ERROR\n";
}
if ( $opts{z} ) {
   $dst->{dbh}->commit if $dst;
   $src->{dbh}->commit;
}
if ( $opts{P} ) {
   printf("%19s %7d %7d\n", ts($now), $now - $start, $cnt);
}

# ############################################################################
# Subroutines.
# ############################################################################

# Repeatedly retries the code until retries runs out, a really bad error
# happens, or it succeeds.  This sub uses lots of global variables; I only wrote
# it to factor out some repeated code.
sub do_with_retries {
   my ( $code ) = @_;
   my $retries = 1; # TODO
   my $success = 0;
   while ( !$success && $retries >= 0 ) {
      eval {
         $code->();
         $success = 1;
      };
      if ( $EVAL_ERROR ) {
         if ( $EVAL_ERROR =~ m/Lock wait timeout exceeded|Deadlock found/ ) {
            if (
               $opts{z} > 1                                    # More than one row per txn
               && $txn_cnt                                     # Not first row
               && (!$can_retry || $EVAL_ERROR =~ m/Deadlock/)  # And it's not retry-able
            ) {
               # The whole txn, which is more than 1 statement, was rolled
               # back.  Must throw away everything and start the transaction
               # over.
               $get_sth->finish;
               $dst->{dbh}->rollback;
               $src->{dbh}->rollback;
               # I wish: $file->rollback
               $get_sth->execute(@beginning_of_txn);
               next ROW;
            }
            # else {
            #  Only one statement had trouble, and the rest of the txn was not
            #  rolled back.  The statement can be retried.
            # }
            --$retries;
         }
         else {
            die;
         }
      }
   }
   return $success;
}

# Formats a row the same way SELECT INTO OUTFILE does by default.  This is
# described in the LOAD DATA INFILE section of the MySQL manual,
# http://dev.mysql.com/doc/refman/5.0/en/load-data.html
sub escape {
   my ($row) = @_;
   return join("\t", map {
      s/([\t\n\\])/\\$1/g if defined $_;  # Escape tabs etc
      defined $_ ? $_ : '\N';             # NULL = \N
   } @$row);
}

# Compares versions like 5.0.27 and 4.1.15-standard-log
sub version_ge {
   my ( $dbh, $target ) = @_;
   my $version = sprintf('%03d%03d%03d', $dbh->{mysql_serverinfo} =~ m/(\d+)/g);
   return $version ge sprintf('%03d%03d%03d', $target =~ m/(\d+)/g);
}

sub get_tbl_struct {
   my ( $info ) = @_;
   my $ddl = ($info->{dbh}->selectrow_array("SHOW CREATE TABLE $info->{db_tbl}"))[1];
   my @defs = $ddl =~ m/^(\s+`.*?),?$/gm;
   my @cols = map { $_ =~ m/`([^`]+)`/g } @defs;
   my @null = map { $_ =~ m/`([^`]+)`/g } grep { $_ !~ m/NOT NULL/ } @defs;
   my %kdef = ();
   my %keys =
      map {
         my ($name) = $_ =~ m/(PRIMARY|`[^`]*`)/;
         my ($cols) = $_ =~ m/\((.+)\),?$/;
         $name =~ s/`//g;
         $kdef{$name} = $_;
         ($name, [ grep { m/[^,]/ } split('`', $cols) ])
      }
      $ddl =~ m/^  ((?:[A-Z]+ )?KEY .*)$/gm;

   return {
      cols      => \@cols,
      col_hash  => { map { $_ => 1 } @cols },
      null      => \@null,
      null_hash => { map { $_ => 1 } @null },
      keys      => \%keys,
      kdef      => \%kdef,
      defs      => \@defs,
      ddl       => $ddl,
   };
}

sub parse_dsn {
   my ($dsn, $prev) = @_;
   return unless $dsn;
   $prev ||= {};

   my %hash = map { m/^(.)=(.*)$/g } split(/,/, $dsn);
   my %vals = map { $_ => $hash{$_} || $prev->{$_} } keys %conn;
   return \%vals;
}

sub get_dbh {
   my ( $info ) = @_;
   my $db_options = {
      AutoCommit => !$opts{z},
      RaiseError => 1,
      PrintError => 1,
   };
   my $dsn = 'DBI:mysql:' . ( $info->{D} || '' ) . ';'
      . join(';', map  { "$conn{$_}=$info->{$_}" } grep { defined $info->{$_} } qw(F h P S))
      . ';mysql_read_default_group=mysql';
   return DBI->connect($dsn, @{$info}{qw(u p)}, $db_options )
      or die("Can't connect to DB: $OS_ERROR");
}

sub ts {
   my ( $time ) = @_;
   my ( $sec, $min, $hour, $mday, $mon, $year )
      = localtime($time);
   $mon  += 1;
   $year += 1900;
   return sprintf("%d-%02d-%02dT%02d:%02d:%02d",
      $year, $mon, $mday, $hour, $min, $sec);
}

sub get_irot {
   my ( $dbh ) = @_;
   return 1 unless version_ge($dbh, '5.0.13');
   my $rows = $dbh->selectall_arrayref(
      "show variables like 'innodb_rollback_on_timeout'",
      { Slice => {} });
   return $rows && @$rows && $rows->[0]->{Value} ne 'OFF';
}

# ############################################################################
# Documentation.
# ############################################################################

=pod

=head1 OPTIONS

=over

=item --file

Filename to write archived rows to.  A subset of MySQL's DATE_FORMAT()
formatting codes are allowed in the filename, as follows:

   %d    Day of the month, numeric (01..31)
   %H    Hour (00..23)
   %i    Minutes, numeric (00..59)
   %m    Month, numeric (01..12)
   %s    Seconds (00..59)
   %Y    Year, numeric, four digits

You can use the following extra format codes too:

   %D    Database name
   %t    Table name

Example:

   --file '/var/log/archive/%Y-%m-%d-%D.%t'

The file's contents are in the same format used by SELECT INTO OUTFILE, as
documented in the MySQL manual: rows terminated by newlines, columns
terminated by tabs, NULL characters are represented by \N, and special
characters are escaped by \.  This lets you reload a file with LOAD DATA
INFILE's default settings.

If you want a column header at the top of the file, see --header.  The file is
auto-flushed by default; see --buffer.

=item --dest

This item specifies a table into which mysql-archiver will insert rows
archived from --source.  It uses the same key=val argument format as --source.
As a shortcut, any values not given default to the same values as --source, so
you don't have to repeat everything.

=item --source

Specifies a table to archive from.  This argument is specially formatted as a
key=value,key=value string.  Keys are a single letter, as follows:

   KEY MEANING
   === =======
   h   Connect to host
   P   Port number to use for connection
   S   Socket file to use for connection
   u   User for login if not current user
   p   Password to use when connecting
   D   Database to archive
   t   Table to archive
   F   Only read default options from the given file
   i   Index to scan while archiving

The only required part is the table; other parts may be read from various
places in the environment (such as options files).  Here is an example:

   --source h=my_server,D=my_database,t=my_tbl

The --dest option uses the same format.

=back

=cut
