#!/usr/bin/perl

# This program efficiently synchronizes data between two MySQL tables, which
# can be on different servers.
#
# This program is copyright (c) 2007 Baron Schwartz, baron at xaprb dot com.
# Feedback and improvements are welcome.
#
# THIS PROGRAM IS PROVIDED "AS IS" AND WITHOUT ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF
# MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.
#
# This program is free software; you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, version 2; OR the Perl Artistic License.  On UNIX and similar
# systems, you can issue `man perlgpl' or `man perlartistic' to read these
# licenses.
#
# You should have received a copy of the GNU General Public License along with
# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
# Place, Suite 330, Boston, MA  02111-1307  USA.

use strict;
use warnings FATAL => 'all';

use DBI;
use English qw(-no_match_vars);
use Getopt::Long;
use List::Util qw(sum max);
use POSIX qw(ceil);

our $VERSION = '@VERSION@';

$OUTPUT_AUTOFLUSH = 1;

my ( $sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst ) = localtime(time);

# ############################################################################
# Get configuration information.
# ############################################################################

my @opt_spec = (
   { s => 'algorithm|a=s',    d => 'Algorithm to use (topdown, bottomup)' },
   { s => 'analyze|A!',       d => 'Analyze (find/fix/print) in bottom-up algorithm (default)' },
   { s => 'branchfactor|B=i', d => 'Branch factor for bottom-up algorithm (default 128)' },
   { s => 'build|U!',         d => 'Build tables for bottom-up algorithm (default)' },
   { s => 'cleanup|C!',       d => 'Clean up scratch tables for bottom-up algorithm (default)' },
   { s => 'columns|c=s',      d => 'Comma-separated column list' },
   { s => 'debug|b',          d => 'Print debugging output to STDOUT' },
   { s => 'queries|q',        d => 'Make debugging output executable SQL' },
   { s => 'drilldown|d=s',    d => 'Drilldown groupings for top-down algorithm' },
   { s => 'engine|E=s',       d => 'Storage engine for bottom-up tables' },
   { s => 'execute|x',        d => 'Execute queries required to sync' },
   { s => 'forupdate|f',      d => 'Use SELECT FOR UPDATE or LOCK IN SHARE MODE for checksums' },
   { s => 'help',             d => 'Show this help message' },
   { s => 'lock|k!',          d => 'Lock tables when beginning work' },
   { s => 'maxcost|m=i',      d => 'Maximum rowcount before bailing out' },
   { s => 'onlydo|o=s',       d => 'Only do INS/UPD/DEL (default: all)' },
   { s => 'prefix|P=s',       d => 'Tablename prefix for bottom-up algorithm' },
   { s => 'print|p',          d => 'Print all sync queries to STDOUT' },
   { s => 'replicate|r',      d => 'Change on master. Implies --strategy=r'},
   { s => 'separator|e=s',    d => 'Separator for CONCAT_WS' },
   { s => 'singletxn|1',      d => 'Do in a single transaction' },
   { s => 'size|S=i',         d => 'Table size in bottom-up algorithm, not usually needed' },
   { s => 'strategy|s=s',     d => 'Query strategy when syncing (r=replace, s=ins/upd/del)' },
   { s => 'temp|T',           d => 'Use temporary tables in bottom-up algorithm (default: no)' },
   { s => 'timeoutok|t',      d => 'Keep going if --wait fails' },
   { s => 'verbose|v+',       d => 'Explain differences found; specify up to three times' },
   { s => 'wait|w=i',         d => 'Make slave wait for master pos (implies --lock)' },
);

# Generate a quasi-random string that's constant in a given day
my $prefix = '__cmp' . crypt(sprintf('%d%02d%02d', $year + 1900, $mon, $mday), '00');
$prefix =~ s/\W+/_/g;

# This is the container for the command-line options' values to be stored in
# after processing.  Initial values are defaults.
my %opts = (
   a => 'topdown',
   e => '#',
   o => 'INSUPDDEL',
   s => 's',
   B => 128,
   P => $prefix,
   E => 'InnoDB',
   U => 1,
   A => 1,
   v => 0,
   1 => 0,
);
my %opt_seen;
foreach my $spec ( @opt_spec ) {
   my ( $long, $short ) = $spec->{s} =~ m/^(\w+)(?:\|([^!+=]*))?/;
   $spec->{k} = $short || $long;
   $spec->{l} = $long;
   $spec->{t} = $short;
   $spec->{n} = $spec->{s} =~ m/!/;
   $opts{$spec->{k}} = undef unless defined $opts{$spec->{k}};
   die "Duplicate option $spec->{k}" if $opt_seen{$spec->{k}}++;
}

# ############################################################################
# Get and process options
# ############################################################################

Getopt::Long::Configure('no_ignore_case', 'bundling');
GetOptions( map { $_->{s} => \$opts{$_->{k}} } @opt_spec) or $opts{help} = 1;

$opts{help} = 1 if $opts{s} && $opts{s} !~ m/^(s|r)/;
$opts{k} ||= $opts{w} if !defined $opts{k};

# Make comma-separated lists into hashes.
if ( $opts{c} ) {
   $opts{c} = { map { $_ => 1 } split(/,\s*/, $opts{c}) };
}
$opts{T} = $opts{T} ? 'TEMPORARY' : '';
if ( !defined $opts{C} ) {
   $opts{C} = !$opts{T};
}
if ( $opts{r} ) {
   $opts{s} = 'r';
}
$opts{o} = lc(join('|', $opts{o} =~ m/(\w{3})/g));

# ############################################################################
# Parse arguments saying which tables.  If the script doesn't have everything
# it needs, show help text.
# ############################################################################
my $source = parse_dsn(shift(@ARGV), {}     );
my $dest   = parse_dsn(shift(@ARGV), $source);

if ( $opts{help} || !$dest ) {
   print "Usage: $PROGRAM_NAME [OPTION].. <source> <dest>\n\n";
   foreach my $spec ( sort { $a->{l} cmp $b->{l} } @opt_spec ) {
      my $long  = $spec->{n} ? "[no]$spec->{l}" : $spec->{l};
      my $short = $spec->{t} ? "-$spec->{t}" : '';
      printf("  --%-13s %-4s %s\n", $long, $short, $spec->{d});
   }
   print <<USAGE;

$PROGRAM_NAME finds and resolves data differences between two MySQL tables.
<source> and <dest> are data sources in the format
   user:pass\@host:port/database.table:key
Everything but the table name is optional, and defaults will be read from your
environment and your .my.cnf file if possible.  Values for <dest> default to the
values for <source>.

For more details, please read the documentation:

   perldoc $PROGRAM_NAME

USAGE
   exit(1);
}

# ############################################################################
# Lookup tables
# ############################################################################
# TODO: another algorithm: binary search for where the corruption starts, say a
# chunk at a time of 10000 from start/end of table, then when a different chunk
# is found, binary search within that chunk to find the first row and report it.
# TODO: grouping strategy: first chars of char column.
# TODO: grouping strategy: number modulo const (between is more easily optimized)

my %code_for_algorithm = (
   topdown  => \&topdown,
   bottomup => \&bottomup,
);

# ############################################################################
# Do the work.
# ############################################################################
my $exit_status = 0;

foreach my $table ( $source, $dest ) {
   my $dbh = get_dbh($table);
   $table->{dbh} = $dbh;
   $table->{db_tbl} =
      join('.',
      map  { $dbh->quote_identifier($_) }
      grep { $_ }
      map  { $_ =~ s/(^`|`$)//g; $_; }
      grep { $_ }
      ( $table->{D}, $table->{t} ));
   $table->{info} = get_tbl_struct($table);
   $table->{cols} = col_list(@{$table->{info}->{cols}});
}

# User wants us to lock for consistency.  But only lock on source initially;
# might have to wait for the slave to catch up before locking on the dest.
if ( $opts{k} ) {
   if ( $opts{v} ) {
      print "-- Locking table on source\n";
   }
   my $lock_mode = $opts{r} ? 'WRITE' : 'READ';
   my $query = "LOCK TABLES $source->{db_tbl} $lock_mode";
   debug_print($query);
   $source->{dbh}->do($query);
   if ( $opts{v} ) {
      print "-- Acquired table lock on source\n";
   }
}

if ( $opts{w} ) {
   my $query = 'SHOW MASTER STATUS';
   debug_print($query);
   my $ms = $source->{dbh}->selectrow_hashref($query);
   if ( $opts{v} ) {
      print "-- Waiting for destination to catch up in binlog\n";
   }
   $query = "SELECT MASTER_POS_WAIT('$ms->{File}', $ms->{Position}, $opts{w})";
   debug_print($query);
   my $stat = $dest->{dbh}->selectall_arrayref($query)->[0]->[0];
   $stat = 'NULL' unless defined $stat;
   if ( $stat eq 'NULL' || $stat < 0 && !$opts{t} ) {
      die "MASTER_POS_WAIT failed: $stat";
   }
   if ( $opts{v} ) {
      print "-- Result of waiting: $stat\n";
   }
}

# Now lock on dest if desired (see above).  In this case don't lock at all on
# destination if it's a replication slave, or the replication thread will be
# locked out... we assume there is nothing else writing on the slave.
if ( $opts{k} && !$opts{r} ) {
   if ( $opts{v} ) {
      print "-- Locking table on destination\n";
   }
   my $query = "LOCK TABLES $dest->{db_tbl} READ";
   debug_print($query);
   $dest->{dbh}->do($query);
   if ( $opts{v} ) {
      print "-- Acquired table lock on destination\n";
   }
}

if ( $code_for_algorithm{$opts{a}} ) {
   $code_for_algorithm{$opts{a}}->();
}
else {
   die "Sorry, algorithm '$opts{a}' unknown; try one of "
      . join('|', keys %code_for_algorithm);
}

foreach my $table ( $source, $dest ) {
   my $dbh = $table->{dbh};
   $dbh->commit if $opts{k} || $opts{f} || $opts{1};
   $dbh->disconnect;
}

exit $exit_status;

# ############################################################################
# Top-down algorithm
# ############################################################################

sub topdown {

   # Design a grouping strategy: user-defined, then finally primary key.
   my @groupings = { cols => $source->{info}->{keys}->{$source->{k}} };
   if ( $opts{d} ) {
      push @groupings, reverse map { { cols => [$_] } } $opts{d} =~ m/(\w+)/g;
   }

   # Array indices
   my ($WHERE, $LEVEL, $COUNT) = (0, 1, 2);

   # Queue of groups to drill into on next iteration.  Managing as a queue, not
   # stack, is breadth-first search, not depth-first.
   my @to_examine = [ {}, $#groupings, 0 ]; # TODO --where applies here

   # Lists of rows that differ in the target tables.
   my (@to_update, @to_delete, @to_insert);

   # Counters
   my %count = map { $_ => 0 } qw(ins upd del bad);

   do {
      my $work  = shift @to_examine;
      my $level = $work->[$LEVEL];
      my $where = $work->[$WHERE];

      my $grouping = $groupings[$level]->{cols};
      my $src_sth  = td_fetch_level($source, $level, $grouping, $where, 'source');
      my $dst_sth  = td_fetch_level($dest,   $level, $grouping, $where, 'dest');

      my ($sr, $dr);       # Source row, dest row
      my %this_level = ( rows => 0, cnt => 0 );

      # The statements fetch in order, so use a 'merge' algorithm of advancing
      # after rows match.  This is essentially a FULL OUTER JOIN.
      MERGE:
      while ( 1 ) { # Exit this loop via 'last'

         if ( !$sr ) {
            $sr = $src_sth->fetchrow_hashref;
         }
         if ( !$dr ) {
            $dr = $dst_sth->fetchrow_hashref;
         }

         # Compare the rows if both exist.  The result is used several places.
         my $cmp;
         if ( $sr && $dr ) {
            $cmp = key_cmp($source, $sr, $dr, $grouping);
         }

         last MERGE unless $sr || $dr;

         my %new_where = %$where;  # Will get more cols added and used below.

         # If the current row is the "same row" on both sides...
         if ( $sr && $dr && defined $cmp && $cmp == 0 ) {
            # The "same" row descends from parents that differ.
            if ( $sr->{__crc} ne $dr->{__crc} ) {
               @new_where{@$grouping} = @{$sr}{@$grouping};
               if ( $level ) {
                  # Special case: push $level - 1 because this will be processed
                  # later.
                  push @to_examine, [ \%new_where, $level - 1, $sr->{__cnt} ];
                  $this_level{cnt}++;
                  $this_level{rows} += $sr->{__cnt};
                  if ( $level && $opts{v} > 2 ) {
                     printf("-- Level %1d: CHECK  group of  %5d rows %s\n",
                        $level, $sr->{__cnt}, make_where_clause($source->{dbh}, \%new_where));
                  }
               }
               else {
                  push @to_update, \%new_where;
                  $count{upd}++;
                  $count{bad}++;
                  if ( $opts{v} > 2 ) {
                     printf("-- Level %1d: UPDATE              1 row  %s\n",
                        $level, make_where_clause($source->{dbh}, \%new_where));
                  }
               }
            }
            $sr = $dr = undef;
         }

         # The row in the source doesn't exist at the destination
         elsif ( !$dr || ( defined $cmp && $cmp < 0 ) ) {
            @new_where{@$grouping} = @{$sr}{@$grouping};
            push @to_insert, \%new_where;
            $count{ins} += $sr->{__cnt} || 1;
            $count{bad} += $sr->{__cnt} || 1;
            if ( $level && $opts{v} > 2 ) {
               printf("-- Level %1d: INSERT group of  %5d rows %s\n",
                  $level, $sr->{__cnt}, make_where_clause($source->{dbh}, \%new_where));
            }
            $sr = undef;
         }

         # Symmetric to the above
         elsif ( !$sr || ( defined $cmp && $cmp > 0 ) ) {
            @new_where{@$grouping} = @{$dr}{@$grouping};
            push @to_delete, \%new_where;
            $count{del} += $dr->{__cnt} || 1;
            $count{bad} += $dr->{__cnt} || 1;
            if ( $level && $opts{v} > 2 ) {
               printf("-- Level %1d: DELETE group of  %5d rows %s\n",
                  $level, $dr->{__cnt}, make_where_clause($source->{dbh}, \%new_where));
            }
            $dr = undef;
         }

         else {
            die "This code should never have run.  This is a bug.";
         }

         if ( $level < $#groupings && $opts{m} && $opts{m} < $count{bad}) {
            print "-- Level $level halt: $count{bad} rows, --maxcost=$opts{m}\n";
            return 0;
         }

      }

      if ( $opts{v} ) {
         printf("--          Level %1d total:   %5d bad rows      %5d to inspect\n",
            $level, $count{bad}, sum(map { $_->[$COUNT] } @to_examine) || 0);
      }
      if ( $opts{v} > 1 ) {
         printf("--          Level %1d summary: %5d bad groups in %5d src groups %5d dst groups\n",
            $level, scalar(@to_examine), $src_sth->rows, $dst_sth->rows);
         printf("--          Level %1d changes: %5d updates       %5d inserts    %5d deletes\n",
            $level, scalar(@to_update), $count{ins}, $count{del});
      }

      $level--;
   } while ( @to_examine );

   # Release locks/close transaction as soon as possible.
   if ( $opts{r} && $opts{f} && !$opts{1} ) {
      $dest->{dbh}->commit;
   }

   td_handle_data_change('INSERT', @to_insert);
   td_handle_data_change('DELETE', @to_delete);
   td_handle_data_change('UPDATE', @to_update);
}

# TODO: group together UPDATE/DELETE whose first key is the same, e.g.
# DELETE FROM tbl WHERE `account` = '733' AND `ikey` = '332286601';
# DELETE FROM tbl WHERE `account` = '733' AND `ikey` = '332286661';
# DELETE FROM tbl WHERE `account` = '733' AND `ikey` = '332287681';
# can be grouped with AND ikey in(....)
sub td_handle_data_change {
   my ( $action, @rows ) = @_;
   return unless $action =~ m/$opts{o}/i;
   foreach my $where ( @rows ) {
      # TODO I'm worried this is double-fetching rows that need to be
      # mass-inserted or updated.
      handle_data_change($action, $where);
   }
}

sub td_fetch_level {
   my ( $info, $level, $groupby, $where, $which ) = @_;
   my $dbh = $info->{dbh};
   my $tbl = $info->{info};

   # Columns that need to be in the checksum list.
   my @cols = grep { !exists($where->{$_}) } @{$tbl->{cols}};
   if ( $opts{c} ) {
      @cols = grep { exists($opts{c}->{$_}) } @cols;
   }
   my $cols = col_list(@cols);

   # To handle nulls, make a bitmap of nullable columns that are null.
   my @null = grep { $tbl->{null_hash}->{$_} } @cols;
   my $null = @null
            ? (", CONCAT(" . join(', ', map  { "ISNULL(`$_`)" } @null) . ")")
            : '';

   my $grp  = col_list(@$groupby);
   my $crit = make_where_clause($dbh, $where);
   my $lock = '';

   if ( $opts{f} && !$opts{k} ) { # User wants us to lock for consistency.
      # Is this the server where changes will happen?
      my $is_target = $opts{r} ? $which eq 'source' : $which eq 'dest';
      $lock = $is_target ? ' FOR UPDATE' : ' LOCK IN SHARE MODE';
   }

   # Maxia's approach used SUM() as the aggregate function.  This is not a good
   # aggregate function; though it commutes, and is therefore order-independent,
   # the law of large numbers will cause checksum collisions on large data sets.
   # BIT_XOR() is really just a bitwise parity.  It is also order-independent,
   # but you expect any given bit in the result to be essentially a random coin
   # flip over the group.
   my $query;
   if ( $level ) {
      ( $query = <<"      END" ) =~ s/\s+/ /g;
         SELECT /*$which:$info->{db_tbl}*/
            $grp,
            COUNT(*) AS __cnt,
            CONCAT(
               LPAD(CONV(BIT_XOR(
                  CAST(CONV(LEFT(\@CRC := MD5(CONCAT_WS('$opts{e}', $cols$null)), 16), 16, 10) AS UNSIGNED)
                  ), 10, 16), 16, '0'),
               LPAD(CONV(BIT_XOR(
                  CAST(CONV(RIGHT(\@CRC, 16), 16, 10) AS UNSIGNED)
                  ), 10, 16), 16, '0')
            ) AS __crc
         FROM $info->{db_tbl}
         $crit
         GROUP BY $grp
         ORDER BY $grp$lock
      END
   }
   else {
      $query = "SELECT /*$which:$info->{db_tbl}*/ $grp, "
         . "MD5(CONCAT_WS('$opts{e}', $cols$null)) AS __crc "
         . "FROM $info->{db_tbl} $crit "
         . "ORDER BY $grp$lock";
   }
   debug_print($query);

   my $sth = $dbh->prepare($query);
   $sth->execute();
   return $sth;
}

# ############################################################################
# Bottom-up algorithm
# ############################################################################

sub bottomup {

   # Ensure branch factor is a power of two.
   $opts{B} = max(2, 2 ** round( log($opts{B}) / log(2) ));

   # Store table prefix in hashes
   $source->{prefix} = "$opts{P}_s_";
   $dest->{prefix}   = "$opts{P}_d_";

   my $levels = 0;
   if ( $opts{U} ) {

      # Begin with estimates of table size to allow calculating the checksum
      # remainder on the first level.
      my $est_size    = $opts{S} || max( estimate_size($source), estimate_size($dest) );
      my $level_est_1 = bu_num_levels($est_size);

      # Determine the data type needed for the remainder column.
      my $rem_col        = bu_size_to_type(( $opts{B} ** ($level_est_1 + 2)) - 1);
      $source->{rem_col} = $rem_col;
      $dest->{rem_col}   = $rem_col;

      # Build the initial checksum tables and calculate how many summary tables to build.
      my $src_size    = bu_build_checksum( $source, $level_est_1 );
      my $level_est_2 = bu_num_levels( max( $est_size, $src_size ) );
      my $dst_size    = bu_build_checksum( $dest, $level_est_2 );
      my $true_size   = max( $src_size, $dst_size );
      $levels         = bu_num_levels( $true_size );

      # Similar to the above, choose a type for the __cnt columns
      my $cnt_col        = bu_size_to_type($true_size);
      $source->{cnt_col} = $cnt_col;
      $dest->{cnt_col}   = $cnt_col;

      # Check and possibly rebuild remainders.
      if ( $levels > $level_est_1 + 2 ) {
         # The initial estimated number of levels caused the first-level tables to
         # have too-small data types, and I don't want to run ALTER TABLE; I'd
         # rather ask the user to re-run.
         die "Table size estimates ($est_size) were too small; specify --size $true_size";
      }
      if ( $level_est_1 != $levels ) {
         bu_rebuild_remainder($source, $levels);
      }
      if ( $level_est_2 != $levels ) {
         bu_rebuild_remainder($dest, $levels);
      }

      # Build the trees, merge them, and clean them up. TODO this part can be
      # parallelized with fork.
      bu_build_tree($source, $levels);
      bu_build_tree($dest,   $levels);
   }
   else {
      $levels  = bu_existing_levels( $source );
   }

   my $finished_work = 1;
   if ( $opts{A} ) {
      $finished_work = bu_merge_tree($dest,   $source, $levels);
   }
   bu_cleanup_tree($dest, $source) if $finished_work && $opts{C};
}

# Builds the first-level checksum table and returns the number of rows in it.
# The bitwise & operator in the __rem calculation is essentially the same as
# MOD().  In unsigned arithmetic, num MOD 128 is the same as num & 127.  It has
# the advantage of taking the absolute value of the modulo though, so there will
# be no negative values.
sub bu_build_checksum {
   my ($info, $levels) = @_;
   my $dbh    = $info->{dbh};
   my $tbl    = $info->{info};
   my $pk     = col_list( @{ $tbl->{keys}->{$info->{k}} } );
   my @cols   = @{ $tbl->{cols} };
   my $cols   = col_list(@cols);
   my $pks    = join( ',', @{ $tbl->{defs} }{ @{ $tbl->{keys}->{$info->{k}} } } );
   my @null   = grep { $tbl->{null_hash}->{$_} } @cols;
   my $null = @null
            ? ( ", CONCAT(" . join( ', ', map {"ISNULL(`$_`)"} @null ) . ")" ) : '';
   my $name = "$info->{prefix}_0";
   my $mask = ($opts{B} ** ($levels - 1)) - 1;

   # Create the table
   my $query = "DROP TABLE IF EXISTS `$name`";
   debug_print($query);
   $dbh->do($query);
   ( $query = <<"   END") =~ s/\s+/ /g;
      CREATE $opts{T} TABLE `$name` (
         $pks,
         __crc CHAR(32) NOT NULL,
         __rem $info->{rem_col} UNSIGNED NOT NULL,
         KEY(__rem),
         PRIMARY KEY($pk)
      ) ENGINE=$opts{E}
   END
   debug_print($query);
   $dbh->do($query);

   # Populate it
   ( $query = <<"   END") =~ s/\s+/ /g;
      INSERT /*$info->{db_tbl}*/ INTO `$name`($pk, __crc, __rem)
      SELECT $pk,
         MD5(CONCAT_WS('$opts{e}', $cols$null)) AS __crc,
         CAST(CONV(RIGHT(MD5(CONCAT_WS('$opts{e}', $pk)), 16), 16, 10) AS UNSIGNED) & $mask AS __rem
      FROM $info->{db_tbl}
   END
   debug_print($query);
   my $sth = $dbh->prepare($query);
   $sth->execute();
   return $sth->rows;
}

sub bu_rebuild_remainder {
   my ( $info, $levels ) = @_;
   my $pk   = col_list( @{ $info->{info}->{keys}->{$info->{k}} } );
   my $mask = ($opts{B} ** ($levels - 1)) - 1;
   my $name = "$info->{prefix}_0";
   my $query = "UPDATE `$name` SET __rem = "
      . "CAST(CONV(RIGHT(MD5(CONCAT_WS('$opts{e}', $pk)), 8), 16, 10) AS UNSIGNED) & $mask";
   debug_print($query);
   $info->{dbh}->do($query);
}

# Builds the nth-level summary tables.
# TODO: allow to use other hash functions like SHA1, and genericize the substringing code
# and the required size of the columns.
sub bu_build_tree {
   my ($info, $levels) = @_;
   my $dbh = $info->{dbh};
   my $tbl = $info->{info};

   # Do from 1 because level 0 has already been built.
   foreach my $i ( 1 .. $levels ) {
      my $modulo   = int($opts{B} ** ( $levels - $i - 1 ));
      my $last_mod = $modulo * $opts{B};
      my $this_tbl = "$info->{prefix}_" . $i;
      my $last_tbl = "$info->{prefix}_" . ( $i - 1 );
      my $mask     = max(0, $modulo - 1);
      my $cnt_sum  = $i > 1 ? 'SUM(__cnt)' : 'COUNT(*)';

      # Create the table
      my $query = "DROP TABLE IF EXISTS `$this_tbl`";
      debug_print($query);
      $dbh->do($query);
      ( $query = <<"      END" ) =~ s/\s+/ /g;
         CREATE $opts{T} TABLE `$this_tbl` (
            __par INT NOT NULL,
            __crc CHAR(32) NOT NULL,
            __rem $info->{rem_col} UNSIGNED NOT NULL,
            __cnt $info->{cnt_col} UNSIGNED NOT NULL,
            KEY(__rem),
            PRIMARY KEY(__par)
         ) ENGINE=$opts{E}
      END
      debug_print($query);
      $dbh->do($query);

      # Populate it
      ( $query = <<"      END" ) =~ s/\s+/ /g;
         INSERT /*$info->{db_tbl}*/ INTO `$this_tbl`
            (__par, __crc, __rem, __cnt)
         SELECT __rem,
            CONCAT(
               LPAD(CONV(BIT_XOR(CAST(CONV(SUBSTRING(__crc, 1,  16), 16, 10) AS UNSIGNED)), 10, 16), 16, '0'),
               LPAD(CONV(BIT_XOR(CAST(CONV(SUBSTRING(__crc, 17, 16), 16, 10) AS UNSIGNED)), 10, 16), 16, '0')
            ) AS this_crc,
            __rem & $mask AS this_remainder,
            $cnt_sum AS total_rows
         FROM `$last_tbl`
         GROUP BY __rem
         ORDER BY NULL
      END
      debug_print($query);
      $dbh->do($query);
   }
}

# There are actually 1 more than $levels summary tables; there are tables 0 ..
# $levels (see bu_build_tree).  Level 0 has a different structure.  It has
# primary keys instead of a __par pointer.
# Returns true if it finished working.
sub bu_merge_tree {
   my ($dest, $source, $levels) = @_;

   my $level = $levels;
   my @bad_parents; # List of parents that must differ at current level
   my ( $rows_in_src, $rows_in_dst ) = (0,0);

   # Lists of rows that differ in the target tables.
   my (@to_update, @to_delete, @to_insert);
   my (@bulk_insert, @bulk_delete);

   # Counters
   my %count = map { $_ => 0 } qw(ins upd del bad);

   do {
      my $src_sth = bu_fetch_level($source, $level, @bad_parents);
      my $dst_sth = bu_fetch_level($dest,   $level, @bad_parents);

      # Reset for next loop, once used to fetch this loop
      @bad_parents = ();
      $rows_in_src = $rows_in_dst = 0;

      my @key = $level ? '__par' : @{$source->{info}->{keys}->{$source->{k}}};
      my ($sr, $dr); # Source row, dest row

      # The statements fetch in order, so use a 'merge' algorithm of advancing
      # after rows match.  This is essentially a FULL OUTER JOIN.
      MERGE:
      while ( 1 ) { # Exit this loop via 'last'

         if ( !$sr ) {
            $sr = $src_sth->fetchrow_hashref;
            if ( $sr ) {
               $rows_in_src += $sr->{__cnt} || 1;
            }
         }
         if ( !$dr ) {
            $dr = $dst_sth->fetchrow_hashref;
            if ( $dr ) {
               $rows_in_dst += $dr->{__cnt} || 1;
            }
         }

         # Compare the rows if both exist.  The result is used several places.
         my $cmp;
         if ( $sr && $dr ) {
            $cmp = key_cmp($source, $sr, $dr, \@key);
         }

         last MERGE unless $sr || $dr;

         # If the current row is the "same row" on both sides...
         if ( $sr && $dr && defined $cmp && $cmp == 0 ) {
            # The "same" row descends from parents that differ.
            if ( $sr->{__crc} ne $dr->{__crc} ) {
               if ( $level ) {
                  push @bad_parents, $sr;
                  if ( $level && $opts{v} > 2 ) {
                     printf("-- Level %1d UPDATE parent:   %5d\n",
                        $level, $sr->{__par});
                  }
               }
               else {
                  $count{upd}++;
                  $count{bad}++;
                  push @to_update, $sr;
               }
            }
            $sr = $dr = undef;
         }

         # The row in the source doesn't exist at the destination
         elsif ( !$dr || ( defined $cmp && $cmp < 0 ) ) {
            if ( $level ) {
               push @bulk_insert, $sr;
               if ( $level && $opts{v} > 2 ) {
                  printf("-- Level %1d BULKIN parent:   %5d\n",
                     $level, $sr->{__par});
               }
            }
            else {
               push @to_insert, $sr;
               if ( $level && $opts{v} > 2 ) {
                  printf("-- Level %1d INSERT parent:   %5d\n",
                     $level, $sr->{__par});
               }
            }
            $count{ins} += $sr->{__cnt} || 1;
            $count{bad} += $sr->{__cnt} || 1;
            $sr = undef;
         }

         # Symmetric to the above
         elsif ( !$sr || ( defined $cmp && $cmp > 0 ) ) {
            if ( $level ) {
               push @bulk_delete, $dr;
               if ( $level && $opts{v} > 2 ) {
                  printf("-- Level %1d BULKDE parent:   %5d\n",
                     $level, $dr->{__par});
               }
            }
            else {
               push @to_delete, $dr;
               if ( $level && $opts{v} > 2 ) {
                  printf("-- Level %1d DELETE parent:   %5d\n",
                     $level, $dr->{__par});
               }
            }
            $count{del} += $dr->{__cnt} || 1;
            $count{bad} += $dr->{__cnt} || 1;
            $dr = undef;
         }

         else {
            die "This code should never have run.  This is a bug.";
         }

         if ( $level < $levels && $opts{m} && $opts{m} < $count{bad} ) {
            print "-- Level $level halt: $count{bad} rows, --maxcost=$opts{m}\n";
            return 0;
         }

      }

      my $sum_bulk_ins = sum(map { $_->{__cnt} } @bulk_insert) || 0;
      my $sum_bulk_del = sum(map { $_->{__cnt} } @bulk_delete) || 0;
      my $sum_parents  = sum(map { $_->{__cnt} || 1 } @bad_parents) || 0;
      my $num_bad_rows = scalar(@to_update) + scalar(@to_insert) + $sum_bulk_ins
                       + scalar(@to_delete) + $sum_bulk_del + $sum_parents;

      if ( $opts{v} ) {
         printf("--         Level %1d total:   %5d rows\n", $level, $num_bad_rows);
      }
      if ( $opts{v} > 1 ) {
         printf("--         Level %1d summary: %5d parents %5d src rows %5d dst rows\n",
            $level, scalar(@bad_parents), $rows_in_src, $rows_in_dst);
         printf("--         Level %1d changes: %5d updates %5d inserts  %5d deletes %5d total\n",
            $level, scalar(@to_update), scalar(@to_insert) + $sum_bulk_ins,
            scalar(@to_delete) + $sum_bulk_del,
            scalar(@to_update) + scalar(@to_insert) + $sum_bulk_ins
               + scalar(@to_delete) + $sum_bulk_del
         );
         printf("--         Level %1d bulk-op: %5d inserts %5d ins-rows %5d deletes %5d del-rows\n",
            $level, scalar(@bulk_insert), $sum_bulk_ins,
            scalar(@bulk_delete), $sum_bulk_del);
      }

      $level--;
   } while ( $level >= 0 && @bad_parents );

   bu_handle_data_change('UPDATE', @to_update);
   bu_handle_data_change('INSERT', @to_insert);
   bu_handle_data_change('DELETE', @to_delete);
   bu_handle_bulk_change('INSERT', $levels, $source, @bulk_insert);
   bu_handle_bulk_change('DELETE', $levels, $dest,   @bulk_delete);

   return 1; # Finished the work.
}

sub bu_cleanup_tree {
   my @servers = @_;
   foreach my $info ( @servers ) {
      my @tables = @{$info->{dbh}->selectcol_arrayref('SHOW TABLES')};
      foreach my $table ( grep { m/^$info->{prefix}_\d+$/ } @tables ) {
         my $query = "DROP TABLE IF EXISTS `$table`";
         debug_print($query);
         $info->{dbh}->do($query);
      }
   }
}

# Finds atomic rows that got folded into an entirely insertable or deleteable
# part of the tree.
sub bu_handle_bulk_change {
   my ( $action, $levels, $info, @rows ) = @_;
   return unless $action =~ m/$opts{o}/i;
   my $pk = col_list( @{ $info->{info}->{keys}->{$info->{k}} } );
   my @rows_to_do;
   my $mask = ($opts{B} ** ($levels - 1)) - 1;

   foreach my $row ( @rows ) {

      # TODO: optimization.
      # This is logically correct, but MySQL won't use indexes:
      # "SELECT $pk FROM $info->{prefix}_0 WHERE __rem & $mask = $row->{__par}"
      # This ends up looking like __rem & 255 = 3.  This will match any of the
      # following (partial list):
      # +-------+--------+
      # | __rem | binary |
      # +-------+--------+
      # |     3 |     11 |
      # |    11 |   1011 |
      # |    15 |   1111 |
      # |    19 |  10011 |
      # |    31 |  11111 |
      # |    51 | 110011 |
      # |    59 | 111011 |
      # +-------+--------+
      # Notice the rightmost two bits are the same in each number.  All these
      # combinations can be generated by adding 3 and every number from 4 to the
      # maximum possible __rem value.  This is easiest to do by mentally
      # left-shifting by the appropriate number of digits and adding.  Suppose
      # $levels is such that the maximum __rem is 63; something like
      # $i = 1; while ( $i * 4 < 63 ) { print 3 + $i * 4; $i++; }
      # If the list is really long, it'll be less efficient for MySQL, so I'd
      # say only do this if the list is less than 20% of the number of __rem
      # values.

      my $parent = $row->{__par};
      my $query  = "SELECT $pk FROM $info->{prefix}_0 WHERE __rem & $mask = $parent";
      debug_print($query);
      my $vals = $info->{dbh}->selectall_arrayref($query, { Slice => {} });
      push @rows_to_do, @$vals;
   }

   bu_handle_data_change($action, @rows_to_do);
}

sub bu_handle_data_change {
   my ( $action, @rows ) = @_;
   return unless $action =~ m/$opts{o}/i;

   foreach my $row ( @rows ) {
      delete $row->{__crc}; # Now the row can be used as a WHERE clause
      handle_data_change($action, $row);
   }
}

sub bu_fetch_level {
   my ( $info, $level, @bad_parents ) = @_;
   my $dbh = $info->{dbh};
   my $tbl = "$info->{prefix}_" . $level;

   my $cols  = $level
             ? '__par, __cnt'
             : col_list( @{ $info->{info}->{keys}->{$info->{k}} } );
   my $where = @bad_parents
             ? "WHERE __rem IN(" . join(',', map { $_->{__par} } @bad_parents) . ")"
             : '';
   my $order = $level
             ? '__par'
             : col_list( @{ $info->{info}->{keys}->{$info->{k}} } );

   my $query = "SELECT $cols, __crc FROM $tbl $where ORDER BY $order";
   debug_print($query);
   my $sth = $dbh->prepare($query);
   $sth->execute();
   return $sth;
}

# Returns how many levels of tables you need to build for a table of a given
# size.  If your B factor is 4 and you pass in 100, you need the summaries
# to be grouping mod 64, 16, 4, 1 so you need 4 levels (5 total including 0,
# which is row-for-row with the real table).
sub bu_num_levels {
   my ( $size ) = @_;
   return int( log($size) / log($opts{B}) );
}

# Returns the maximum modulus that the tables will need.
sub bu_size_to_type {
   my ( $size ) = @_;
   return $size < 256        ? 'TINYINT'
        : $size < 65536      ? 'SMALLINT'
        : $size < 16777216   ? 'MEDIUMINT'
        : $size < 4294967296 ? 'INT'
        :                      'BIGINT';
}

# Figure out how many levels exist for pre-existing tables.
sub bu_existing_levels {
   my ($info) = @_;
   my @tables = @{$info->{dbh}->selectcol_arrayref("SHOW TABLES")};
   @tables    = grep { m/^$info->{prefix}_\d+$/ } @tables;
   die "No existing tables with prefix $info->{prefix} found" unless @tables;
   return max(map { $_ =~ m/(\d+)$/g } @tables);
}

# ############################################################################
# Subroutines
# ############################################################################

# NULL sorts before defined values in MySQL, so I consider undef "less than."
# Otherwise I compare strings case-insensitively even though this will cause
# problems in some cases.
# TODO: figure out when things should be compared with a _cs or _bin
# collation.  Maybe this is impossible.
sub key_cmp {
   my ( $info, $r1, $r2, $key ) = @_;
   foreach my $i ( 0 .. @$key - 1 ) {
      my $l = $r1->{$key->[$i]};
      my $r = $r2->{$key->[$i]};
      if ( defined $l && defined $r ) {
         my $n   = $key->[$i] eq '__par' || $info->{info}->{num_hash}->{$key->[$i]}; # Whether numeric col
         my $cmp = $n ? $l <=> $r : lc $l cmp lc $r;
         return $cmp unless $cmp == 0;
      }
      elsif ( defined $l ) {
         return 1;
      }
      elsif ( defined $r ) {
         return -1;
      }
   }
   return 0;
}

# All output has to be prefixed with SQL comments so the output can be piped
# right into MySQL if desired.
sub debug_print {
   return unless $opts{b};
   print '-- ' if !$opts{q};
   print @_;
   print ";\n" if $opts{q};
   print "\n" if $opts{b};
}

# Code factored out of bu_handle_data_change and td_handle_data_change.
sub handle_data_change {
   my ( $action, $where) = @_;
   my $which = $opts{r} ? $source : $dest;
   my $dbh   = $which->{dbh};
   my $crit  = make_where_clause($dbh, $where);

   if ( $action eq 'DELETE' ) {
      my $query = "DELETE FROM $which->{db_tbl} $crit";
      if ( $opts{p} ) {
         print STDOUT $query, ";\n";
      }
      if ( $opts{x} ) {
         $dbh->do($query);
      }
   }

   else {
      my $query = "SELECT $source->{cols} FROM $source->{db_tbl} $crit";
      debug_print($query);
      my $sth = $source->{dbh}->prepare($query);
      $sth->execute();
      while ( my $res = $sth->fetchrow_hashref() ) {
         if ( $opts{s} eq 'r' || $action eq 'INSERT' ) {
            my $verb = $opts{s} eq 'r' ? 'REPLACE' : 'INSERT';
            $query = "$verb INTO $which->{db_tbl}($which->{cols}) VALUES("
               . join(',', map { $dbh->quote($res->{$_}) }
                  @{$which->{info}->{cols}}) . ")";
         }
         else {
            my @cols = grep { !exists($where->{$_}) } @{$which->{info}->{cols}};
            $query = "UPDATE $which->{db_tbl} SET "
               . join(',',
                  map { $dbh->quote_identifier($_) . '=' .  $dbh->quote($res->{$_}) } @cols)
               . ' ' . $crit;
         }
         if ( $opts{p} ) {
            print STDOUT $query, ";\n";
         }
         if ( $opts{x} ) {
            $dbh->do($query);
         }
      }
   }
}

# Parses a DSN in login:pass@host:port/database.table:key format
sub parse_dsn {
   my ($dsn, $prev) = @_;
   return unless $dsn;
   $prev ||= {};

   my ( $user, $pass, $host, $port, $database, $table, $key) = $dsn =~ m{
      (?:
         (.+?)       # Username
         (?::(.+))?  # Optional password
      @)?            # User-pass is optional
      (?:
         (.+?)       # Hostname
         (?::(.+))?  # Optional port
      /)?            # Host-port is optional
      (?:
         (.+?)       # Database
      \.)?           # Database is optional
      ([^:]+)        # Table is required
      (?::
         (.+)        # Key/index
      )?             # Index is optional
      }xsm or return undef;

   return {
      u => $user     || $prev->{u},
      p => $pass     || $prev->{p},
      h => $host     || $prev->{h},
      P => $port     || $prev->{P},
      D => $database || $prev->{D},
      t => $table    || $prev->{t},
      k => $key      || $prev->{k} || 'PRIMARY',
   };
}

sub get_tbl_struct {
   my ( $info ) = @_;
   my $ddl = ($info->{dbh}->selectrow_array("SHOW CREATE TABLE $info->{db_tbl}"))[1];
   my @defs = $ddl =~ m/^(\s+`.*?),?$/gm;
   my @cols = map { $_ =~ m/`([^`]+)`/g } @defs;
   my @nums = map  { $_ =~ m/`([^`]+)`/g }
              grep { $_ =~ m/`[^`]+` (?:(?:tiny|big|medium|small)?int|float|double|decimal)/ } @defs;
   my @null = map { $_ =~ m/`([^`]+)`/g } grep { $_ !~ m/NOT NULL/ } @defs;
   my %keys =
      map {
         my ($name) = $_ =~ m/(PRIMARY|`[^`]*`)/;
         my ($cols) = $_ =~ m/\((.+)\),?$/;
         $name =~ s/`//g;
         ($name, [ grep { m/[^,]/ } split('`', $cols) ])
      }
      $ddl =~ m/^  ((?:[A-Z]+ )?KEY .*)$/gm;
   my %alldefs;
   @alldefs{@cols} = @defs;

   if ( !exists $keys{$info->{k}} ) {
      die "No such key $info->{k} in table $info->{h}/$info->{db_tbl}";
   }

   return {
      cols      => \@cols,
      col_hash  => { map { $_ => 1 } @cols },
      null      => \@null,
      null_hash => { map { $_ => 1 } @null },
      keys      => \%keys,
      defs      => \%alldefs,
      nums      => \@nums,
      num_hash  => { map { $_ => 1 } @nums },
   };
}

# Get a size estimate (not a precise count because that may be very slow).
# Can't use COUNT(*) because it might be optimized away and so 'rows' could be
# null.  And in tables where there is no NULL column, again it could be
# optimized away, so I must generate a WHERE clause that will defeat this.
sub estimate_size {
   my ($info) = @_;
   my ( $pkcol ) = @{$info->{info}->{keys}->{$info->{k}}};
   my $query = "EXPLAIN SELECT COUNT("
      . join("), COUNT(", split(',', col_list(@{$info->{info}->{cols}})))
      . ") FROM $info->{db_tbl} "
      . "WHERE COALESCE(`$pkcol`, `$pkcol`) = `$pkcol`";
   debug_print($query);
   return $info->{dbh}->selectrow_hashref($query)->{rows};
}

sub round {
   my ($number) = @_;
   return int( $number + .5 );
}

sub make_where_clause {
   my ( $dbh, $where ) = @_;
   my @clauses = map {
      my $val = $where->{$_};
      my $sep = defined $val ? ' = ' : ' IS ';
      $dbh->quote_identifier($_) . $sep . $dbh->quote($val);
   } keys %$where;
   my $clause = @clauses ? 'WHERE ' . join(' AND ', @clauses) : '';
   return $clause;
}

sub make_key {
   my ( $row, $cols ) = @_;
   return join('#', map { defined $row->{$_} ? $row->{$_} : 'NULL' } @$cols);
}

sub get_dbh {
   my $db_options = {
      AutoCommit => !$opts{k} && !$opts{f} && !$opts{1},
      RaiseError => 1,
      PrintError => 1,
   };
   my ( $info ) = @_;
   my %conn = ( h => 'host', P => 'port', S => 'socket');
   my $dsn = 'DBI:mysql:' . ( $info->{D} || '' ) . ';'
      . join(';', map  { "$conn{$_}=$info->{$_}" } grep { defined $info->{$_} } qw(h P S))
      . ';mysql_read_default_group=mysql';
   return DBI->connect($dsn, @{$info}{qw(u p)}, $db_options )
      or die("Can't connect to DB: $OS_ERROR");
}

sub col_list {
   return '`' . join('`,`', @_) . '`';
}

sub coalesce {
   my $i = 0;
   while ( $i < @_ && !defined $_[$i] ) {
      $i++;
   }
   return $_[$i];
}

# ############################################################################
# Documentation
# ############################################################################
=pod

=head1 NAME

mysql-table-sync - Efficiently synchronize data between two MySQL tables.

=head1 SYNOPSIS

To compare two tables,

 mysql-table-sync -a topdown -d col1,col2 user:pass@host1/db.tbl user@host2/db.tbl

Or,

 mysql-table-sync -a bottomup -B 128 user:pass@host1/db.tbl user@host2/db.tbl

To show the differences between the tables, use the --verbose option.  Issue
this option multiple times for more detail.  To see queries that will make the
second table the same as the first, use the --print option.  To execute these
queries, use the --execute option.  To see the queries issued while searching
for differences, use the --debug option.

=head1 DESCRIPTION

I wrote this tool to help me resync slaves that "drift" from their masters,
which can happen for any number of reasons.  I wanted a solution that would work
well for MySQL replication, so I didn't have to re-initialize the slaves, which
can be prohibitively expensive -- if there's enough data, even stopping and
restarting the slave is costly, as it takes a while to "warm up" the server.
Add to that the overhead of copying huge amounts of data over the network, and
the time involved, and a way to resync the slaves "live" is very attractive
indeed.  There are also many constraints introduced by replication, which I
wanted to either avoid or use to my advantage.

I know not everyone has exactly these needs, so I made the tool much more
generic than I'd need to patch a table that's out of sync on a slave.

The DBA must choose the algorithm and parameters to use when reconciling
differences between the tables.  See below for help making this decision.
Different algorithms have more or less network traffic, impact on the servers,
or work better in certain circumstances.  The tool supports a variety of
algorithms so you can resolve the differences as efficiently as possible within
whatever parameters matter to you.

=head1 SYSTEM REQUIREMENTS

You need Perl, DBI, DBD::mysql, and some core packages that ought to be
installed in any reasonably new version of Perl.

=head1 OVERVIEW

This tool implements two algorithms to find differences between two MySQL
tables, which need not be on the same server.  One is "bottom-up" and builds
summary tables from each table, then traverses them to find rows and chunks of
rows that differ.  The other is "top-down" and builds no summary tables, but
repeatedly queries the target table.  Each algorithm has strengths and
limitations, and is suitable for different situations.

Once you've identified the differences, you can also choose from several methods
of resolving them.  One method is to do inserts, updates and deletes to the
destination table.  The other assumes the destination table is on a replication
slave and makes the changes on the source (master) server, counting on
replication to propagate the changes.  Again it's up to a smart DBA to decide
which method is best.

There are also variations on all the techniques, in support of locking,
master/slave consistency, partial-row updates, and so forth.

=head1 FINDING DIFFERENCES BOTTOM-UP

The bottom-up method of finding differences begins by checksumming every row in
the source and destination target tables.  The result of the checksum is stored
in a scratch table on the server.

This scratch table is what I call the "level 0" table.  It contains the target
table's primary key columns and a checksum of all the columns, concatenated.
This checksum is easy to compare and makes it possible to see whether the rows
differ.  Level 0 contains one row per row in the target table.

Table "level 1" is derived from level 0 by grouping a number of rows together
and checksumming the group.  How many rows are grouped together is up to the
user, but it must be a power of two.  128 is a suggestion I've seen.  I refer to
this number as the "branch factor" because the summary tables conceptually build
a tree.

The grouping works by dividing the checksum of each row in level 0 by some power
of the branch factor and taking the remainder.  A checksum is a number, though
it is usually written as a string of hex digits, such as
acbd18db4cc2f85cedef654fccc4a4d8, so you can divide and take the remainder
(modulus) easily.  (This tool actually uses some bitwise arithmetic to optimize
this, but I won't go into it here).  The power of the branch factor decreases as
the levels are built, so the remainders get smaller and smaller, grouping the
rows into fewer and fewer summary rows.

Level 1's primary key is not the target table's primary key.  It is the modulus
of the group from which the row was derived.  For example, if a number of rows
in level 0 have a modulus of 11, they will be grouped together into a single row
in level 1, with the primary key value of 11.

Assuming a branch factor of 128, level 1 has 1/128th as many rows as level 0,
give or take.  Level 2 is built from level 1, and has 1/128th as many rows in it
again, and so on until level N, which has just a single row.

After building scratch tables 0 through N, the tool begins at level N and works
backwards.  At level N, there is just a single row.  If the checksum in this row
matches on source and destination, the tables must be identical, and there is no
more work to do.  If they differ though, some rows in level N-1 must differ, and
the tool examines the "parent" rows in level N-1.  It continues to do this until
it travels all the way back "up the tree" to level 0 and identifies exactly
which rows in the target tables are different.  It uses breadth-first search.

I've glossed over many subtleties.  For example, as the summary tables are
built, not only their checksums but the remainders are computed on the fly; the
remainders are stored in the summary tables and are indexed for efficient lookup
as the algorithm traverses back up the tree seeking differences.  A running
count is also stored so at any given point you know how many rows in the target
table got rolled into the one row you're examining, no matter what level you're
at in the tree.  Some optimizations can be used to short-circuit the process
when entire chunks of rows are missing from one of the tables, and so on.  But
these optimizations and subtleties are just efficiencies, and are not necessary
for correctness.

Here are some details about the table structures: The __crc column contains the
checksum of the row from which it was derived.  The __cnt column contains the
running count of rows from which it was derived, except in the case of level 0
where each row is derived from one row.  The __rem column contains the checksum
modulus the power of the branching factor.  The __par column in level N is a
"pointer" to the __rem column in level N-1.

=head1 FINDING DIFFERENCES TOP-DOWN

The top-down algorithm is nearly the reverse of the bottom-up algorithm.
Instead of building summary tables bottom-up from many rows and ending with one
row, then searching top-down back through the summaries, it does an n-ary search
on clusters of the tables, which I refer to as "groupings."  The search begins
with grouped data and ends with single rows, instead of beginning with single
rows and working towards summaries.  There are no summary tables.

The basic idea is to choose an appropriate grouping strategy which will allow
MySQL to use indexes to drill down through regions of the table, grouping each
region together at first and comparing whole chunks of data between the source
and destination.  Suppose the tables contain day-level data for many client
accounts; clients can have many accounts.  Day, client, account and whatever the
primary key is, are all indexed.  The drill-down strategy might first group the
table by day and see which days differ between the tables, then within the days
that differ group by client, then account, and finally descend to the individual
row level, using the primary key.

This approach is also a breadth-first search as I've implemented it.  At each
level in the drill-down, the tool knows a set of truths, such as the value of
certain columns in the rows, the number of rows that might be bad, and so on.
In this respect it is fairly similar to the bottom-up approach.

=head1 PROS AND CONS OF THE TWO SEARCH METHODS

Each method for finding the rows that differ has its own strengths and
weaknesses, and is suited for different scenarios.

The bottom-up approach has these advantages:

=over

=item *

The checksum and modulo arithmetic ensure a uniform hierarchy of rows in the
"tree" of N-level summary tables.  Taking the modulo of a checksum is
essentially a random number, which will distribute rows approximately evenly in
each successive summary.

=item *

The algorithm makes no assumptions about keys or data types, and will work on
any table with a primary key or a user-specified index.  All you need is a way
to identify a row, in the final analysis.

=item *

The technique works the same on every table structure; there's no need to think
about the "best" way to do it for a specific table.

=item *

The summary tables can be kept and re-used for successive analyses, or to
restart an analysis that fails for some reason (for example, you specified a
maximum cost before halting, and it was exceeded but you've changed your mind;
no need to rebuild the summaries, you can just restart).

=item *

If you assume there are occasional "bad" rows scattered through the table, the
entire tree of summary tables will need to be examined to find them.
Pre-calculating this is an up-front penalty that pays off in efficiency once you
try to find the bad rows.

=item *

This algorithm's best and worst cases, in terms of pre-computing the summary
tables, are identical.  Given that you know the table size, you know how
expensive it will be.

=item *

Parts of the algorithm can be parallelized readily, though I have not yet done
so, as I want to make sure the implementation is correct first (I plan to do
this soon).

=item *

This algorithm is network-efficient, as the potentially large rows in the target
tables (suppose each row has very large BLOBS in it) are not sent across the
network.  Only the checksums are sent across the network, until the bad rows
themselves are identified.

=back

The algorithm has its shortcomings too, though:

=over

=item *

It is necessary to do some possibly significant work up front to design the
summary tables properly.  For example, you need to know the maximum possible
number of rows to be examined.  I have tried to optimize this process as much as
possible by examining index statistics and making estimates.  This is fairly
cheap, and should work in nearly every case, but it makes the coding much more
complex, just to avoid things like a COUNT(*) query, which is notoriously slow
on InnoDB.

=item *

All the summary tables may add up to a LOT of data on very large tables.  If the
target table is narrow, the summary tables may be even larger than the target
tables, though there will never be more rows in level 0 than the target table
has.

=item *

It's hard to make this approach play nicely with replication.  If you build
temporary tables in memory, you're playing havoc with statement-based
replication should the slave crash.  Even if you build them on disk, which is
durable and restartable, the summary tables built on the master will replicate
to the slave.  The slave server will be doing double work with the master's
queries running on it.  Either way, building summary tables on the server is
anathema to replication.

=item *

It's hard to lock the table for consistency, should you wish to.  You can't
design the summary tables until you start querying the target tables, yet you
can't write to the summary tables while only holding a lock on the target table,
which would require releasing and re-acquiring the lock on the target -- race
conditions abound.  I do have a workaround to this problem in mind, but have not
implemented it yet (and it would not solve the problem in cases where the
destination table is being written to).

=item *

The original algorithm, as designed by Coelho, didn't use any indexes or
pre-computed and cached remainders and counts on the summary tables.  This is
extremely inefficient on large tables, causing repeated table scans.  While I
have modified the algorithm to avoid this, it comes at the cost of larger table
and index size on the server.

=item *

The checksum/modulo approach destroys locality of reference in the target
tables.  Suppose the rows that differ between the source and destination tables
are concentrated in a small region of the table; the checksum/modulo math will
randomly scatter these neighbors throughout the summary tables.  This precludes
some types of optimizations.  This is very important on extremely large tables,
as it causes lots of random I/O during the search phase.  It's also a realistic
scenario for large tables, which may tend to be append-only logs or similar
(credit card transactions, for example).

=item *

The reverse of something I mentioned earlier as a benefit becomes a drawback
when only a small part of the table is bad.  Building the entire summary table
tree for just one bad row is wasteful.  Most of the tree will compare equal, but
there's no chance for early optimizations by pruning those branches; they've
already been built by the time the search notices they aren't needed.

=item *

All the INSERT .. SELECT statements necessary to build the summary tables will
acquire shared row locks on InnoDB tables.  This overhead adds up.

=back

The top-down approach is quite different, both in behavior and implementation
details.  Here are some of its strengths:

=over

=item *

This approach is network-efficient.  Large rows are not sent across the network,
just as with the bottom-up approach.  However, it is also memory and space
efficient on the servers, as there are no summary tables to build.

=item *

This approach does allow for early optimizations such as tree-pruning in the
search.

=item *

The target table's natural groupings, created by its indexes, can (and should)
be exploited.

=item *

The queries are not replicated because they don't affect any data.  The queries
on the master will not cause extra work on the slave.

=item *

It's easy to lock the tables with one of several strategies, including
intentionally locking InnoDB tables with SELECT FOR UPDATE or SELECT LOCK ON
SHARE MODE, and MySQL's own table locks.

=item *

There is no need in this algorithm to compute branching factors and numbers of
summary levels needed according the the size of the target tables.  There's no
up-front analysis to do.  You don't need to pre-compute anything before
searching for differences.

=item *

Spatial locality can be exploited.  Adjacent "bad" rows stay adjacent during the
search.  This can help avoid random I/O during the search.

=item *

There are more opportunities for bulk operations, such as noticing a large chunk
of the table can be inserted or deleted en masse.  While there are some in the
bottom-up approach, there are not as many, and they're harder to optimize.

=item *

The best case for this algorithm is not the same cost as the worst case.  This
can be good or bad, depending on the scenario.

=item *

The grouping strategy determines the "branch factor" of the search tree, and it
can vary from level to level in the search, depending on the data and the
strategy chosen.  This can be a good thing in the hands of a smart DBA, or a bad
thing in a novice's hands.

=item *

As the search descends through the tree of groupings, it needs to examine fewer
and fewer columns in the target tables.

=back

The top-down approach is not without its weaknesses:

=over

=item *

The search strategy can be simpler or more complex.  It's up to the DBA.  There
are certainly more possibilities, and choosing a good top-down strategy might be
hard.

=item *

If there's no locality to exploit, the top-down approach might not be able to
prune the search tree, and it might end up doing more work than the bottom-up.

=item *

The drill-down must repeatedly checksum the entire row (except the columns it
holds as constants in each level of the search).  If the rows are very large,
for example if there are large text columns, this might be a lot of work for the
server to do over and over.

=back

=head1 RESOLVING DIFFERENCES

Once the tool has found the differences, you probably want to resolve them.
There are two major ways I know to do this, and several variations.

The most obvious way is to simply issue INSERT, UPDATE, and DELETE statements
against the destination server.

A more subtle approach is to take advantage of replication and issue the
statements on the master, letting replication propagate them to the slave.  In
this case a slightly different approach is needed.  If a row is missing on the
slave, you can't just INSERT it on the master, or you'll presumably get a
duplicate key error.  You could do INSERT IGNORE or REPLACE instead.

The tool uses REPLACE by default when you're using replication.  To tell it to
use INSERT and UPDATE instead, use the --strategy=s option.  You can use
--strategy=s to use REPLACE even when you're not using replication to fix the
destination table.

You can also use the --onlydo option to only issue some kinds of statements --
for example, suppose you want to run the INSERT and UPDATE statements but not
the DELETE.

=head1 GUARANTEEING CONSISTENCY

I wrote this tool to synchronize tables live, without stopping the servers
they're on.  This requires some kind of locking to guarantee a consistent write
after reading.  This tool supports several methods.

The first is simple table locks.  If you specify the --lock option, it will lock
the table for reading or writing, depending on how you want to update.  If you
want to update on a master and let replication propagate the changes, it locks
for write on the master and read on the slave; if you want to make changes on
the slave, it locks for read on the master and doesn't lock on the slave, to
avoid blocking the replication thread on the slave.

If you are using InnoDB tables, you can get consistency without locking the
whole table.  This is especially useful if you're only trying to synchronize
part of the table that you know to be bad.  Use the --forupdate option to make
the SELECT statements acquire locks.  As above, it gets either shared (LOCK IN
SHARE MODE) or exclusive (FOR UPDATE) locks, depending on how you are syncing.
This only applies to the top-down algorithm.  The bottom-up algorithm creates
tables, which implicitly commits and releases locks unless you use temporary
tables (the --temp option).  If you're using temporary tables with the bottom-up
algorithm, there are implicit shared locks on the target tables if they're
InnoDB.

Finally, if you're working on a master and slave server, you should probably
specify the --wait option in conjunction with one of the above. This locks on
the master, finds the master's position, and then waits for the slave to catch
up to that position.  The argument to the option is the number of seconds the
slave should wait before timing out.  By default --wait implies --lock, but you
can specify --nolock if you want to override this and use InnoDB's row-level
locks.  Also by default, if the wait timeout is exceeded or another
MASTER_POS_WAIT error occurs, the program will exit with an error, but you can
use the --timeoutok option to control this.

=head1 OUTPUT

Output varies greatly depending on the command-line options you specify.
There are several different kinds of output: debugging, status, and query.  I
have tried to ensure it will always be valid SQL, though much of it will be
commented out.

If you specify the --print option, the queries needed to sync the destination
table with the source table will be printed to STDOUT.

If you specify the --debug option, the queries needed to discover the
differences between source and destination will be printed to STDOUT,
commented out.  You can use this to see how many queries are executed for a
given strategy, or whatever other debugging you want.  If you remove the
comment characters, you can also replay the process of finding the
differences.

If you specify the --verbose option, you'll see information about the process of
discovering the differences between the tables.  The output is quite different
for top-down and bottom-up algorithms.  Specify this option multiple times to
increase the amount of information you see.  This output is complex enough
that I'll need to explain it separately.

=head2 TOP-DOWN OUTPUT

At its most verbose, the top-down output may resemble this:

   -- Level 2: CHECK  group of    81 rows WHERE `col2` = '20'
   -- Level 2: INSERT group of    18 rows WHERE `col2` = '42'
   --          Level 2 total:     18 bad rows        81 to inspect
   --          Level 2 summary:    1 bad groups in    3 src groups    2 dst groups
   --          Level 2 changes:    0 updates         18 inserts       0 deletes
   -- Level 1: CHECK  group of     6 rows WHERE `col3` = '737696900' AND `col2` = '20'
   -- Level 1: CHECK  group of     1 rows WHERE `col3` = '737953400' AND `col2` = '20'
   -- Level 1: CHECK  group of     1 rows WHERE `col3` = '737955900' AND `col2` = '20'
   --          Level 1 total:     18 bad rows         8 to inspect
   --          Level 1 summary:    3 bad groups in   32 src groups   32 dst groups
   --          Level 1 changes:    0 updates         18 inserts       0 deletes
   --          Level 0 total:     18 bad rows         2 to inspect
   --          Level 0 summary:    2 bad groups in    6 src groups    6 dst groups
   --          Level 0 changes:    0 updates         18 inserts       0 deletes
   -- Level 0: UPDATE             1 row  WHERE `col3` = '737953400' AND `col1` = '87551' AND `col2` = '20'
   --          Level 0 total:     19 bad rows         1 to inspect
   --          Level 0 summary:    1 bad groups in    1 src groups    1 dst groups
   --          Level 0 changes:    1 updates         18 inserts       0 deletes
   --          Level 0 total:     19 bad rows         0 to inspect
   --          Level 0 summary:    0 bad groups in    1 src groups    1 dst groups
   --          Level 0 changes:    1 updates         18 inserts       0 deletes

Outdented lines are actions that must be taken later, indented lines are
play-by-play status as differences between the tables are found.  There will be
one group of indented lines for each group of rows drilled into and found to
have differences.

The first two lines are details of level 2.  At level 2, col2 is held as a
constant.  There is one group of 81 rows where col2 = 20, which does not match
from source to destination.  It needs further checking and is marked as CHECK.
The next level will drill down into this group.  Also at level 2, there is one
group of 18 rows that needs to be inserted to sync the destination table.  This
does not need to be drilled into on the next level.

The next three lines summarize the findings at level 2, and the work that
remains to be done.  Line 1 shows level 2 found a total of 18 rows known to
differ between the source and destination, and there are 81 more to inspect
further.  Line 2 shows level 2 found 1 entire group of rows known to be bad
(the group that must be inserted) after inspecting 3 groups from the source
table and 2 from the destination table.  The difference, 3-2, is the one group
that must be inserted.  Line 3 shows 18 rows have been queued for insertion
en masse.

The next six lines are what happens in the one group at level 1.  At level 1,
the group of rows where col2 = 20 is drilled into, grouped on col3.  The first
three lines of output show the algorithm finds three groups of rows that don't
match.  The next three show the total bad-row count still at 18, so no new bad
rows have been found, but the number of rows that must be drilled into is much
smaller now -- only 8 rows.  Level 1 found 3 bad groups by checking 32 groups
from each table, and queued no new rows into the known-bad list.

The level 0 output shows these 3 groups being examined a row at a time, with no
more drilldown possible.  And you can see the one bad row being found.
Eventually the last line of output shows 18 rows must be inserted (no change
from before) and 1 row must be updated to sync the destination table.

It might help to see what happens with only one level of verbosity, this time on
a 50,000 row table with 5 rows missing from the destination:

   --          Level 2 total:       0 bad rows       6385 to inspect
   --          Level 1 total:       0 bad rows       5142 to inspect
   --          Level 1 total:       1 bad rows       4235 to inspect
   --          Level 1 total:       1 bad rows       2919 to inspect
   --          Level 1 total:       1 bad rows       2560 to inspect
   --          Level 1 total:       1 bad rows        325 to inspect
   --          Level 0 total:       2 bad rows        142 to inspect
   --          Level 0 total:       2 bad rows        141 to inspect
   --          Level 0 total:       3 bad rows         61 to inspect
   --          Level 0 total:       4 bad rows         37 to inspect
   --          Level 0 total:       5 bad rows          0 to inspect

Now you can see it progressing from 0 known bad rows, with 6385 to do, all the
way to 5 known bad rows and 0 left to do.

=head2 BOTTOM-UP OUTPUT

At maximum verbosity, the output from the bottom-up algorithm may look like this
(this output is from the same 100-row tables as above).

   -- Level 2 UPDATE parent:      0
   --         Level 2 total:    100 rows
   --         Level 2 summary:    1 parents  100 src rows   82 dst rows
   --         Level 2 changes:    0 updates    0 inserts     0 deletes    0 total
   --         Level 2 bulk-op:    0 inserts    0 ins-rows    0 deletes    0 del-rows
   -- Level 1 UPDATE parent:      0
   -- Level 1 BULKIN parent:      1
   -- Level 1 UPDATE parent:      2
   -- Level 1 UPDATE parent:      3
   -- Level 1 UPDATE parent:      4
   -- Level 1 UPDATE parent:      5
   -- Level 1 UPDATE parent:      8
   -- Level 1 UPDATE parent:      9
   -- Level 1 UPDATE parent:     10
   -- Level 1 UPDATE parent:     11
   -- Level 1 UPDATE parent:     14
   -- Level 1 UPDATE parent:     15
   --         Level 1 total:     84 rows
   --         Level 1 summary:   11 parents  100 src rows   82 dst rows
   --         Level 1 changes:    0 updates    1 inserts     0 deletes    1 total
   --         Level 1 bulk-op:    1 inserts    1 ins-rows    0 deletes    0 del-rows
   --         Level 0 total:     19 rows
   --         Level 0 summary:    0 parents   83 src rows   66 dst rows
   --         Level 0 changes:    1 updates   18 inserts     0 deletes   19 total
   --         Level 0 bulk-op:    1 inserts    1 ins-rows    0 deletes    0 del-rows

The first level, level 2, says the parent row whose remainder is 0 (this will
always be the case at the first level in bottom-up) differs.  At this point it
looks like the parent must be updated to reconcile source and destination
tables, but it's not yet known which individual rows must be changed.  The level
2 summary says there are 100 rows grouped together from parent rows, that is 1
parent value with 100 rows in the source and 82 in the destination (the
difference is the 18 rows that must be inserted, but that is not yet known).
The next two lines of output show what work is queued to do -- row-level
updates, inserts and deletes, and bulk inserts and deletes.  Each bulk insert or
delete knows how many rows it will affect.

The next set of output, for level 1, shows that this level of the tree has 11
rows that don't match between source and destination.  These are again marked as
UPDATE because they differ, but it's still not known why.  One row doesn't exist
in the destination and is marked as BULKIN, for "bulk insert."  This level of
drill-down was able to narrow the part of the table possibly bad from 100 to 84
rows.

At level 0, this narrows down to just 19 rows.  Most of these are inserted
singly, and there is one update.

Again, here is what happens with just one level of verbosity on the same
50,000-row tables as above:

   --         Level 2 total:   50000 rows
   --         Level 1 total:    1945 rows
   --         Level 0 total:       5 rows

At the beginning, all 50,000 rows look bad, but as it navigates the tree, it
narrows it down to just the 5 missing rows.

=head1 COMPATIBILITY

My goal is a superb solution for MySQL.  However, I think you can probably
make some minor changes and use this tool on other platforms.

=head1 HISTORY AND ACKNOWLEDGEMENTS

My work is based in part on Giuseppe Maxia's work on distributed databases,
L<http://www.sysadminmag.com/articles/2004/0408/> and code derived from that
article.  There is more explanation, and a link to the code, at
L<http://www.perlmonks.org/?node_id=381053>.

Another programmer extended Maxia's work even further.  Fabien Coelho changed
and generalized Maxia's technique, introducing symmetry and avoiding some
problems that might have caused too-frequent checksum collisions.  This work
grew into pg_comparator, L<http://www.coelho.net/pg_comparator/>.  Coelho also
explained the technique further in a paper titled "Remote Comparison of Database
Tables" (L<http://cri.ensmp.fr/classement/doc/A-375.pdf>).

This existing literature mostly addressed how to find the differences between
the tables, not how to resolve them once found.  I needed a tool that would not
only find them efficiently, but would then resolve them.  I first began thinking
about how to improve the technique further with my article
L<http://www.xaprb.com/blog/2007/03/05/an-algorithm-to-find-and-resolve-data-differences-between-mysql-tables/>,
where I discussed a number of problems with the Maxia/Coelho "bottom-up"
algorithm.  After writing that article, I began to write this tool.  I wanted to
actually implement their algorithm with some improvements so I was sure I
understood it completely.  I discovered it is not what I thought it was, and is
considerably more complex than it appeared to me at first.  Fabien Coelho was
kind enough to address some questions over email.

The improvements to the bottom-up algorithm are my original work, as is the
top-down algorithm.  The techniques to actually resolve the differences are also
my own work.

=head1 COPYRIGHT, LICENSE AND WARRANTY

This program is copyright (c) 2007 Baron Schwartz, baron at xaprb dot com.
Feedback and improvements are welcome.

THIS PROGRAM IS PROVIDED "AS IS" AND WITHOUT ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF
MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation, version 2; OR the Perl Artistic License.  On UNIX and similar
systems, you can issue `man perlgpl' or `man perlartistic' to read these
licenses.

You should have received a copy of the GNU General Public License along with
this program; if not, write to the Free Software Foundation, Inc., 59 Temple
Place, Suite 330, Boston, MA  02111-1307  USA.

=head1 AUTHOR

Baron Schwartz, baron at xaprb dot com.

=cut
