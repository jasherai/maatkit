Build a query sniffer that sniffs off the wire and outputs in slow-log format.

Include the DSN options in POD.

Make a way to get a list of DSNs out of a database table, and iterate over them.
This would be useful for mk-find, mk-table-checksum and several other tools.

Tool to automate InnoDB recovery: start mysqld, dump table in chunks until it
crashes, start again, ....

Include --help and --version in POD, make --help show what kind of value is
required

A tool (or a feature on mk-slave-prefetch or mk-heartbeat) to watch how fast a
slave is replicating, in bytes/sec.

Debugging output when daemonizing, like mk-slave-restart has.

Add timezone to DSN syntax

Integrate mk-slave-delay with mk-heartbeat.

Make mk-slave-delay implement a different definition of 'delay' as per Defition
2 in bug report #1959496.

a tool to load tables/indexes into cache by the techniques Peter mentioned:
http://www.mysqlperformanceblog.com/2008/05/01/quickly-preloading-innodb-tables-in-the-buffer-pool/

Make mk-table-checksum checksum only newly inserted rows (as defined by the
checksum table).  Make it able to read from a table that defines tables to
checksum and the desired strategies.

Add a --charset option to all tools, like mk-table-sync has.

Add a --setvars option to all tools, so one can set wait_timeout etc.
Make it set the wait_timeout by default.

mk-log-server: start a server that can serve the given binlog files.

mysqldiff: http://www.adamspiers.org/computing/mysqldiff/
http://www.mysqldiff.org/
(mk-schema-sync or similar tool should be in SVN history)

query sniper:
   * http://www.stillhq.com/mysql/000008.html
   * Allow up to max_connections-X connections
   * Specify users/hosts NOT to kill (by default root/localhost)
   * http://google-mysql-tools.googlecode.com/svn/trunk/mypgrep.py

Log parsing and coloring:

   Swallow a log file and output only the selected fields.  This could be
   piped to a colorizer or pushed into mk-query-profiler.

   We need to update our slow-log tool to tell us the most recent time a query
   was executed

   Query rewriter: collapse INSERT ... VALUES (), (), (), ()..... to reduce the
   number of VALUES lists to just one, with a (VALUES_LIST_REPEATED) token after
   that.

A script to copy files in parallel with netcat.

#!/bin/bash

set -e
set -u

# #######################################################################
# Arguments
# #######################################################################
SRC=$1
DST=$2
SDIR=$3
DDIR=$4
EXCL=$5

running=0
port=12345

function copyfile () {
   file=$1;
   port=$2;
   echo "$file, $port";
   #(sleep 10; touch forked)&
   #while [ ! -e forked ]; do
      #echo "sleeping";
      #sleep 1;
   #done
   #rm forked
}

for file in `ssh $SRC "du $SDIR/*" | grep -v "'$EXCL'" | sort -nr | awk '{print $2}'`; do
   let port=port+1;
   copyfile $file $port;
done

Progress calculations:
   *  Look at PROCESSLIST, grab the query, EXPLAIN it, watch handlers, do the
   * the problem is really it only works for some query types 
   * If second query would be Ref for example you would not find it
   because you can't see first table Ref from second table Ref
   * But in particular case of FT Scan + bunch of refs it is very helpful
   * Yes, true: but, you could calculate from the rows and access types
   how many of each type of Handler operation you expect to see for the whole
   query, right?
   * It would be very hard to do for subqueries, though :)
   * what about progress for mysqldump ?
   * and reverse - mysql < from_dump.sql

Make visual-explain tell which columns are used in the index.

Query formatter/pretty printer

timestamp column names: (updat|insert|delet|creat)ed

New tool to help alter tables in chunks:
   IDEA: We actually could add a tool to maatkit to handle ALTER TABLE
   w replication similar way - create table_new with new structure and
   when lock both tables and populate it with selects from the large
   table in small chunks - this would allow to alter large tables
   without stalling replication flow.
