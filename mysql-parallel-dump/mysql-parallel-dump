#!/usr/bin/perl

# This is a program to dump MySQL tables in parallel, via mysqldump or SELECT
# INTO OUTFILE.
#
# This program is copyright (c) 2007 Baron Schwartz.
# Feedback and improvements are welcome.
#
# THIS PROGRAM IS PROVIDED "AS IS" AND WITHOUT ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF
# MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.
#
# This program is free software; you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, version 2; OR the Perl Artistic License.  On UNIX and similar
# systems, you can issue `man perlgpl' or `man perlartistic' to read these
# licenses.
#
# You should have received a copy of the GNU General Public License along with
# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
# Place, Suite 330, Boston, MA  02111-1307  USA.

use strict;
use warnings FATAL => 'all';

use DBI;
use English qw(-no_match_vars);
use File::Basename qw(dirname);
use File::Spec;
use Getopt::Long;
use List::Util qw(max);
use POSIX;

our $VERSION = '@VERSION@';
our $DISTRIB = '@DISTRIB@';
our $SVN_REV = sprintf("%d", q$Revision: 859 $ =~ m/(\d+)/g);

# ############################################################################
# Get configuration information.
# ############################################################################

# TODO:
# read sets of tables together from a table.
# Check a table for last-modified for each table.

my @opt_spec = (
   { s => 'basedir=s',         d => 'Base directory for files (default cwd)' },
   { s => 'binlogpos|b!',      d => 'Dump the master/slave position (default)' },
   { s => 'csv',               d => 'Do --tab dump in CSV format' },
   { s => 'databases|d=s',     d => 'Only do this comma-separated list of databases' },
   { s => 'dbregex=s',         d => 'Dump only databases whose names match this pattern' },
   { s => 'defaults-file|F=s', d => 'Only read default options from the given file' },
   { s => 'flushlock|k!',      d => 'Use FLUSH TABLES WITH READ LOCK (default)' },
   { s => 'flushlog!',         d => 'Execute FLUSH LOGS (default)' },
   { s => 'gzip!',             d => 'Compress files with gzip (default on non-Win32)' },
   { s => 'host|h=s',          d => 'Connect to host' },
   { s => 'help',              d => 'Show this help message' },
   { s => 'ignoredb|g=s',      d => 'Ignore this comma-separated list of databases' },
   { s => 'ignoretbl|n=s',     d => 'Ignore this comma-separated list of tables' },
   { s => 'locktables',        d => 'Use LOCK TABLES (implies --no-flushlock)' },
   { s => 'numthread|m=i',     d => 'Number of threads (default 2, or #CPUs on Linux)' },
   { s => 'opt!',              d => 'Use sensible mysqldump options (enabled by default)' },
   { s => 'password|p=s',      d => 'Password to use when connecting' },
   { s => 'port|P=i',          d => 'Port number to use for connection' },
   { s => 'quiet|q',           d => 'Do not print normal output' },
   { s => 'sets=s',            d => 'Backup this comma-separated list of sets' },
   { s => 'settable=s',        d => 'DB.Table where backup sets are stored' },
   { s => 'socket|S=s',        d => 'Socket file to use for connection' },
   { s => 'tab|T',             d => 'Dump tab-separated (sets --umask 0)' },
   { s => 'tables|t=s',        d => 'Only do this comma-separated list of tables' },
   { s => 'test',     ,        d => 'Print commands instead of executing them' },
   { s => 'tblregex=s',        d => 'Dump only tables whose names match this pattern' },
   { s => 'umask=s',           d => 'Umask value, in octal' },
   { s => 'user|u=s',          d => 'User for login if not current user' },
   { s => 'version',           d => 'Output version information and exit' },
   { s => 'wait|w=s',          d => 'Wait limit when server is down (default 5m)' },
);

# Holds command-line options.
my %opts = (
   b        => 1,
   opt      => 1,
   gzip     => $OSNAME !~ m/Win32/,
   basedir  => File::Spec->curdir(),
   flushlog => 1,
   w        => '5m',
);

# Post-process...
my %opt_seen;
foreach my $spec ( @opt_spec ) {
   my ( $long, $short ) = $spec->{s} =~ m/^([\w-]+)(?:\|([^!+=]*))?/;
   $spec->{k} = $short || $long;
   $spec->{l} = $long;
   $spec->{t} = $short;
   $spec->{n} = $spec->{s} =~ m/!/;
   $opts{$spec->{k}} = undef unless defined $opts{$spec->{k}};
   die "Duplicate option $spec->{k}" if $opt_seen{$spec->{k}}++;
}

# ############################################################################
# Get options.
# ############################################################################
Getopt::Long::Configure('no_ignore_case', 'bundling');
GetOptions( map { $_->{s} => \$opts{$_->{k}} } @opt_spec) or $opts{help} = 1;

# ############################################################################
# Process options.
# ############################################################################
$opts{basedir} = File::Spec->rel2abs($opts{basedir});

if ( !$opts{m} ) {
   eval {
      # Try to read --numthread from the number of CPUs in /proc/cpuinfo.  This
      # only works on GNU/Linux.
      open my $file, "<", "/proc/cpuinfo"
         or die $OS_ERROR;
      local $INPUT_RECORD_SEPARATOR = undef;
      my $contents = <$file>;
      close $file;
      $opts{m} = scalar( map { $_ } $contents =~ m/(processor)/g );
   };
   $opts{m} ||= $ENV{NUMBER_OF_PROCESSORS}; # MSWin32
   $opts{m} = max(2, $opts{m} || 0);
}

# Set locking options.
my $lock_all      = $opts{set} || $opts{locktables} ? 0 : 1;
$opts{k}          = $lock_all unless defined $opts{k};
$opts{locktables} = !$lock_all unless defined $opts{locktables};

if ( !$opts{help} ) {
   if ( !$opts{m} ) {
      warn "You must specify --numthread\n";
      $opts{help} = 1;
   }

   if ( !$opts{help} && $opts{k} && $opts{locktables} ) {
      warn "--locktables and --flushlock are mutually exclusive\n";
      $opts{help} = 1;
   }

   if ( !$opts{help} ) {
      my ($num, $suf ) = $opts{w} =~ m/(\d+)([smhd])$/;
      if ( !defined $num || $num <= 0 ) {
         warn "Invalid --wait argument\n";
         $opts{help} = 1;
      }
      else {
         $opts{w} = $suf eq 's' ? $num            # Seconds
                  : $suf eq 'm' ? $num * 60       # Minutes
                  : $suf eq 'h' ? $num * 3600     # Hours
                  :               $num * 86400;   # Days
      }
   }

   if ( !$opts{help} && $opts{set} && !$opts{settable} ) {
      warn "--set requires --settable\n";
      $opts{help} = 1;
   }
}

if ( $opts{T} ) {
   if ( !defined $opts{umask} ) {
      $opts{umask} = 0;
   }
}

if ( defined $opts{umask} ) {
   umask oct($opts{umask});
}

# Gather connection parameters to pass to mysqldump.  Order matters; mysqldump
# will have a problem if --defaults-file isn't first.
my @conn_params = (
   [qw(--defaults-file F)],
   [qw(--host          h)],
   [qw(--password      p)],
   [qw(--port          P)],
   [qw(--socket        S)],
   [qw(--user          u)],
);
@conn_params = map { "$_->[0]='$opts{$_->[1]}'" } grep { defined $opts{$_->[1]} } @conn_params;

# Decide on options to mysqldump.
my $default_dir;
my @mysqldump_args;
if ( $opts{opt} && !@ARGV ) {
   # Sensible defaults.
   $default_dir = 1;
   @mysqldump_args = (
      qw(mysqldump),
      @conn_params,
      ( $opts{T} ? qw(--no-data) : () ),
      qw(
         --add-drop-table
         --add-locks
         --allow-keywords
         --comments
         --complete-insert
         --create-options
         --disable-keys
         --extended-insert
         --quick
         --quote-names
         --set-charset
         --skip-lock-tables
         --triggers
         --tz-utc
         %D
         %N
      ),
   );
   if ( $opts{gzip} ) {
      push @mysqldump_args, qw( | gzip --force --fast --stdout - > ),
         filename('%S', '%D', '%N.sql.gz');
   }
   else {
      push @mysqldump_args,
         '--result-file=' . filename('%S', '%D', '%N.sql');
   }
}

if ( $opts{version} ) {
   print "mysql-parallel-dump  Ver $VERSION Distrib $DISTRIB Changeset $SVN_REV\n";
   exit(0);
}

if ( $opts{help} ) {
   print "Usage: mysql-parallel-dump <options> [--] <mysqldump args>\n\n";
   my $maxw = max(map { length($_->{l}) + ($_->{n} ? 4 : 0)} @opt_spec);
   foreach my $spec ( sort { $a->{l} cmp $b->{l} } @opt_spec ) {
      my $long  = $spec->{n} ? "[no]$spec->{l}" : $spec->{l};
      my $short = $spec->{t} ? "-$spec->{t}" : '';
      printf("  --%-${maxw}s %-4s %s\n", $long, $short, $spec->{d});
   }
   (my $usage = <<"   USAGE") =~ s/^      //gm;

      mysql-parallel-dump dumps MySQL tables in parallel via mysqldump or
      SELECT INTO OUTFILE.  If possible, database connection options are read
      from your .my.cnf file.  For more details, please read the
      documentation:

         perldoc mysql-parallel-dump

   USAGE
   print $usage;
   exit(0);
}

# Make comma-separated lists into hashes.
if ( $opts{d} ) {
   $opts{d} = { map { $_ => 1 } split(/,\s*/, $opts{d}) };
}
$opts{g} = { map { $_ => 1 } split(/,\s*/, $opts{g} || '') };
if ( $opts{t} ) {
   $opts{t} = { map { $_ => 1 } split(/,\s*/, $opts{t}) };
}
$opts{n} = { map { $_ => 1 } split(/,\s*/, $opts{n} || '') };
if ( $opts{e} ) {
   $opts{e} = { map { lc($_) => 1 } split(/,\s*/, $opts{e}) };
}

# ############################################################################
# Connect.
# ############################################################################
my %conn = (
   F => 'mysql_read_default_file',
   h => 'host',
   P => 'port',
   S => 'mysql_socket'
);

my $dbh = get_dbh();
$dbh->{InactiveDestroy}  = 1;         # Don't die on fork().
$dbh->{FetchHashKeyName} = 'NAME_lc'; # Lowercases all column names for fetchrow_hashref()

# This signal handler will do nothing but wake up the sleeping parent process.
$SIG{CHLD} = sub {};

# ############################################################################
# Lock the whole server if desired.
# ############################################################################
if ( $opts{k} && !$opts{test} ) {
   $dbh->do('FLUSH TABLES WITH READ LOCK');
}

# ############################################################################
# Iterate over "sets" of tables.
# ############################################################################
my %sets;

# Fetch backup sets from the database.
if ( $opts{sets} ) {
   my $setnames = "'" . join("', '", $opts{set} =~ m/(\w+)/g) . "'";
   my $sql = "SELECT `setname`, `priority`, `db`, `tbl` "
           . "FROM $opts{settable} WHERE `setname` IN($setnames) "
           . "ORDER BY `setname`, `priority`, `db`, `tbl`";
   my $result = $dbh->selectall_arrayref($sql, { Slice => {} } );
   foreach my $row ( @$result ) {
      push @{$sets{$row->{setname}}},
         {  D => $row->{db},
            N => $row->{tbl},
            S => $row->{setname}
         };
   }
}

# Do all databases and tables in a 'default' set.
else {
   my @work_to_do;
   my @databases
      = grep {
         ( !$opts{d} || exists($opts{d}->{$_}) )
         && ( !$opts{dbregex} || $_ =~ m/$opts{dbregex}/ )
         && ( $_ !~ m/^(information_schema|lost\+found)$/mi )
         && ( !exists $opts{g}->{$_} )
      } @{$dbh->selectcol_arrayref('SHOW DATABASES')};

   DATABASE:
   foreach my $database ( @databases ) {
      my $tables = $dbh->selectall_arrayref(
         "SHOW /*!50002 FULL*/ TABLES FROM `$database`");
      if ( @$tables ) {
         TABLE:
         foreach my $table ( @$tables ) {
            my $tblname = $table->[0];
            next TABLE if
               ( $opts{t} && !exists($opts{t}->{$tblname}) )
               || ( $opts{tblregex} && $tblname !~ m/$opts{tblregex}/ )
               || ( @$table == 2 && $table->[1] eq 'VIEW' )
               || exists $opts{n}->{$tblname};
            push @work_to_do,
               {  D => $database,
                  N => $tblname,
                  S => 'default',
               };
         }
      }
   }
   $opts{set}     = 'default';
   $sets{default} = \@work_to_do;
}

# ############################################################################
# Do each backup set.
# ############################################################################
foreach my $set ( $opts{set} =~ m/(\w+)/g ) {
   my @work_to_do = @{$sets{$set}};

   # #########################################################################
   # Lock tables if needed (automatically releases any earlier locks).
   # #########################################################################
   if ( $opts{locktables} && !$opts{test} ) {
      $dbh->do('LOCK TABLES '
         . join(', ', map { "`$_->{D}`.`$_->{N}` READ" } @work_to_do));
   }

   # #########################################################################
   # Flush logs.
   # #########################################################################
   if ( $opts{flushlog} && !$opts{test} ) {
      $dbh->do('FLUSH LOGS');
   }

   # #########################################################################
   # Get the master position.
   # #########################################################################
   if ( $opts{b} && !$opts{test} ) {
      my $filename = filename($set, '00_master_data.sql');
      makedir($filename);
      open my $file, ">", $filename or die $OS_ERROR;
      my %wanted = map { $_ => 1 }
         qw(file position master_host master_port master_log_file
         read_master_log_pos relay_log_file relay_log_pos relay_master_log_file
         exec_master_log_pos);

      my ( $master_pos, $slave_pos );
      eval {
         $master_pos = $dbh->selectrow_hashref('SHOW MASTER STATUS');
      };
      eval {
         $slave_pos = $dbh->selectrow_hashref('SHOW SLAVE STATUS');
         print $file "CHANGE MASTER TO MASTER_HOST='$slave_pos->{master_host}', "
                   . "MASTER_LOG_FILE='$slave_pos->{master_log_file}', "
                   . "MASTER_LOG_POS=$slave_pos->{read_master_log_pos}\n";
      };
      my %hash;
      foreach my $thing ( $master_pos, $slave_pos ) {
         next unless $thing;
         foreach my $key ( grep { $wanted{$_} } sort keys %$thing ) {
            print $file "-- $key $thing->{$key}\n";
         }
      }
      close $file or die $OS_ERROR;
   }

   # #########################################################################
   # Design the format for printing out.
   # #########################################################################
   my ( $maxdb, $maxtbl, $maxset);
   $maxdb  = max(8, map { length($_->{D}) } @work_to_do);
   $maxtbl = max(5, map { length($_->{N}) } @work_to_do);
   $maxset = max(3, length($set));
   my $format = "%-${maxset}s %-${maxdb}s %-${maxtbl}s %5s %6s %7s\n";
   printf($format, qw(SET DATABASE TABLE TIME STATUS THREADS))
      unless $opts{q} || $opts{test};

   # #########################################################################
   # Assign the work to child processes.  Initially just start --numthreads
   # number of children.  Each child that exits will trigger a new one to start
   # after that.  This is really a terrible hack -- I wish Perl had decent
   # threading support so I could just queue work for a fixed pool of worker
   # threads!
   # #########################################################################

   my %kids;
   while ( @work_to_do || %kids ) {

      # Wait for the MySQL server to become responsive.
      my $tries = 0;
      while ( !$dbh->ping && $tries++ < $opts{w} ) {
         sleep(1);
         eval {
            $dbh = get_dbh();
         };
         if ( $EVAL_ERROR ) {
            $EVAL_ERROR =~ s/^.*?failed: (.*?) at \S+ line (\d+).*$/$1 at line $2/s;
            print "$EVAL_ERROR\n";
         }
      }
      if ( $tries >= $opts{w} ) {
         die "Too many retries, exiting.\n";
      }

      # Start a new child process.
      while ( @work_to_do && $opts{m} > keys %kids ) {
         my $todo = shift @work_to_do;
         $todo->{time} = time;
         my $pid = fork();
         die "Can't fork: $OS_ERROR" unless defined $pid;
         if ( $pid ) {              # I'm the parent
            $kids{$pid} = $todo;
         }
         else {                     # I'm the child
            exit(do_table($todo));
         }
      }

      # Possibly wait for child.
      my $reaped = 0;
      foreach my $pid ( keys %kids ) {
         my $kid = waitpid($pid, POSIX::WNOHANG);
         if ( $kid ) { # I reaped a child.
            my $stat = $CHILD_ERROR;
            my $todo = $kids{$kid};
            printf($format, @{$todo}{qw(S D N)}, (time - $todo->{time}), $stat, scalar keys %kids)
               unless $opts{q} || $opts{test};
            delete $kids{$kid};
            $reaped = 1;
         }
      }

      if ( !$reaped ) {
         # Don't busy-wait.  But don't wait forever either, as a child may exit
         # and signal while we're not sleeping, so if we sleep forever we may
         # not get the signal.
         sleep(1);
      }
   }

}

# ############################################################################
# Subroutines
# ############################################################################

# Interpolates % directives from a db/tbl hashref.
sub interp {
   my ( $todo, @strings ) = @_;
   map { $_ =~ s/%([SDN])/$todo->{$1}/g } @strings;
   return @strings;
}

# Actually dumps a table.
sub do_table {
   my ( $todo ) = @_;
   my $exit_status = 0;

   # Dump via SELECT INTO OUTFILE.
   if ( $opts{T} ) {
      my $dbh  = get_dbh();
      my $filename = filename(interp($todo, '%S', '%D', '%N'));
      makedir($filename);
      my $sql;
      if ( $opts{csv} ) {
         $sql  = qq{SELECT * INTO OUTFILE '$filename.txt' }
               . qq{FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\"' }
               . qq{LINES TERMINATED BY '\\n' FROM `$todo->{D}`.`$todo->{N}`};
      }
      else {
         $sql = "SELECT * INTO OUTFILE '$filename.txt' FROM `$todo->{D}`.`$todo->{N}`";
      }
      if ( $opts{test} ) {
         print $sql, "\n";
      }
      $dbh->do($sql) unless $opts{test};
      if ( $opts{gzip} ) {
         $exit_status = system_call('gzip', '--force', '--fast', "$filename.txt");
      }
   }

   # Normal dump using mysqldump.  If $opts{T} was set, the following won't dump
   # the data.  If the user left the options alone, we can predict the filename
   # and directory naming convention, and ensure the directiories exist.
   # Otherwise the user must ensure the directories exist, because it's a hard
   # job to figure out which argument in the array is a filename.
   if ( $default_dir ) {
      makedir(interp($todo, filename('%S', '%D', '%N.sql')));
   }

   my @args = map { interp($todo, $_) } @mysqldump_args;
   $exit_status = system_call( @args ) || $exit_status;
   return $exit_status;
}

# Makes a filename.
sub filename {
   my $filename = File::Spec->catfile($opts{basedir}, @_);
   return $filename;
}

{
   # Memoize...
   my %dirs;

   # If the directory doesn't exist, makes the directory.
   sub makedir {
      my ( $filename ) = @_;
      return if $opts{test};
      my @dirs = File::Spec->splitdir(dirname($filename));
      foreach my $i ( 0 .. $#dirs ) {
         my $dir = File::Spec->catdir(@dirs[0 .. $i]);
         if ( !$dirs{$dir} ) {
            if ( ! -d $dir ) {
               mkdir($dir, 0777);
            }
            $dirs{$dir}++;
         }
      }
   }
}

sub get_dbh {
   my $dsn = 'DBI:mysql:;'
      . join(';', map  { "$conn{$_}=$opts{$_}" } grep { defined $opts{$_} } qw(F h P S))
      . ';mysql_read_default_group=mysql';
   my $dbh = DBI->connect($dsn, @opts{qw(u p)}, { AutoCommit => 1, RaiseError => 1, PrintError => 0 } );
   return $dbh;
}

sub system_call {
   my ( @cmd ) = @_;
   if ( $opts{test} ) {
      print join(' ', @cmd), "\n";
      return 0;
   }
   else {
      return system(join(' ', @cmd));
   }
}

# ############################################################################
# Documentation.
# ############################################################################

=pod

=head1 NAME

mysql-parallel-dump - Dump sets of MySQL tables in parallel.

=head1 SYNOPSIS

  mysql-parallel-dump
  mysql-parallel-dump --tab
  mysql-parallel-dump --sets order,profile,session

=head1 DESCRIPTION

MySQL Parallel Dump connects to a MySQL server, finds database and table names,
and does parallel dumps on them.  It can be used in several pre-packaged ways,
or as a generic wrapper to call some program in parallel, passing it parameters
for each table.

To dump all tables to gzipped files in the current directory, each database with
its own directory, with locks, flushing the binary log, and recording binary log
positions, each table in a single file:

  mysql-parallel-dump

To dump databases and tables elsewhere:

  mysql-parallel-dump --basedir /path/to/elsewhere

To dump to tab-separated files with C<SELECT INTO OUTFILE>, each table with
separate data and SQL files:

  mysql-parallel-dump --tab

To dump one or more backup sets (see L<"BACKUP SETS">):

  mysql-parallel-dump --sets set1,set2,set3

To "write your own command line," use C<--> to indicate where the arguments for
MySQL Parallel Dump stop and where the arguments for C<mysqldump> (or any other
program) begin.  The following example shows C<mysqldump>, and aside from simpler
options to C<mysqldump>, is basically what happens when you specify no arguments
at all:

  mysql-parallel-dump -- mysqldump --skip-lock-tables '%D' '%N' \
     \| gzip --fast -c - \> '%D.%N.gz'

The C<%> modifiers are macros (see L<"MACROS">).  The C<--skip-lock-tables>
argument is very important in that last example, because otherwise both MySQL
Parallel Dump and C<mysqldump> will lock tables, so C<mysqldump> will hang,
waiting for the locks.  Notice the shell metacharacters C<|> and C<E<gt>> are
escaped so the shell won't interpret them, and they'll get passed through to the
generated command-line.

When you use built-in defaults, MySQL Parallel Dump will relay these arguments
on to every forked copy of mysqldump: L<"--defaults-file">, L<"--host">,
L<"--port">, L<"--socket">, L<"--user">, L<"--password">.  If you write your own
command-line, you will need to specify them manually.

MySQL Parallel Dump does I<not> back up your entire database.  It dumps tables and
data I<only>.  It does not dump view definitions or stored routines.  However,
if you dump the C<mysql> database, you'll be dumping the views and stored
routines by default.

=head1 BACKUP SETS

Backup sets are groups of logically related tables you want to backup together.
You specify a set by inserting the table names into a table in the MySQL server
from which you're dumping, and then naming it in the L<"--set"> option.  MySQL
Parallel Dump always works a set at a time; if you don't specify a set, it
auto-discovers tables and considers them the default set.

The table that stores backup sets should have at least these columns: setname,
priority, db, tbl.  The following is a suggested table structure:

  CREATE TABLE backupset (
    setname  CHAR(64) NOT NULL,
    priority INT      NOT NULL DEFAULT 0,
    db       CHAR(64) NOT NULL,
    tbl      CHAR(64) NOT NULL,
    PRIMARY KEY(setname, priority, db, tbl),
    UNIQUE  KEY(setname, db, tbl)
  );

=head1 MACROS

MySQL Parallel Dump can insert % variables into arguments.  The available macros
are as follows:

  MACRO  MEANING
  =====  =================
  %S     The backup set
  %D     The database name
  %N     The table name

=head1 OUTPUT

Each line of output is printed when a forked "child" process ends and is removed
from the list of children.  The output shows the backup set, database, table,
seconds spent dumping, the exit status of the forked dump process, and number of
current processes (including the one just reaped; so this typically shows "how
many are running in parallel").  A status of 0 indicates success:

  SET     DATABASE TABLE         TIME STATUS THREADS
  default mysql    db               0      0       4
  default mysql    columns_priv     0      0       4
  default mysql    help_category    0      0       3

=head1 OPTIONS

Some options can be disabled by prefixing them with C<--no>, such as
C<--no-gzip>.

=over

=item --basedir

The directory in which files will be stored.  If you use pre-canned options,
such as L<"--tab">, MySQL Parallel Dump knows what the eventual filenames will
be, and can place all the files in this directory.  It will also create any
parent directories that don't exist, if needed (see also L<"--umask">).

The default is the current working directory.

If you write your own command line, MySQL Parallel Dump cannot know which
arguments in the command line are filenames, and thus doesn't know the
eventual destination of the dump files.  It does not try to create parent
directories in this case.

=item --binlogpos

Dump binary log positions from both C<SHOW MASTER STATUS> and C<SHOW SLAVE
STATUS>, whichever can be retrieved from the server.  The data is dumped to a
file named F<00_master_data.sql>.  This is done for each backup set.

=item --csv

Changes L<"--tab"> options so the dump file is in comma-separated values
(CSV) format.  The SELECT INTO OUTFILE statement looks like the following, and
can be re-loaded with the same options:

   SELECT * INTO OUTFILE %D.%N.txt
   FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'
   LINES TERMINATED BY '\n' FROM %D.%N;

=item --databases

Dump this comma-separated list of databases.  Only applies when L<"--set"> is
not given.

=item dbregex

Dump only databases whose names match this Perl regular expression.  Only
applies when L<"--set"> is not given.

=item --defaults-file

Only read default options from the given file.  You must give an absolute
pathname.

=item --flushlock

Lock all tables globally with C<FLUSH TABLES WITH READ LOCK>.  This is enabled
by default, unless you're dumping sets (see L<"--sets">).  If you only want to
lock the tables you're dumping, use L<"--locktables">.  This lock is taken
once, at the beginning of the whole process, and is never released.

=item --flushlog

Execute C<FLUSH LOGS> after locking and before dumping master/slave binary log
positions.  Enabled by default.  This is done for each backup set.

=item --gzip

Compresses files with gzip.  This is enabled by default unless your platform
is Win32.  By default, this causes the standard SQL dumps to be piped to
gzip's C<STDIN> and the result is redirected to the destination file.  If this
option isn't enabled, by default mysqldump's C<--result-file> parameter is used
to direct the dump to the destination file.  When using L<"--tab">, this
option causes gzip to be called separately on each resulting file after it is
dumped (because C<SELECT INTO OUTFILE> cannot be directed to a pipe).

=item --help

Displays a help message.

=item --host

Connect to host.

=item --ignoredb

Do not dump this comma-separated list of databases.  Only applies when
L<"--set"> is not given.

=item --ignoretbl

Do not dump this comma-separated list of table (not database.table) names.  Only
applies when L<"--set"> is not given.

=item --locktables

Disables L<"--flushlock"> (unless it was explicitly set) and locks tables with
C<LOCK TABLES READ>.  Enabled by default when L<"--set"> is specified.  The
lock is taken and released with every set of tables dumped.

=item --numthread

Specifies the number of parallel processes to run.  The default is 2 (this is
MySQL Parallel Dump, after all -- 1 is not parallel).  On GNU/Linux machines,
the default is the number of times 'processor' appears in F</proc/cpuinfo>.  On
Windows, the default is read from the environment.  In any case, the default is
at least 2, even when there's only a single processor.

=item --opt

This is sort of related to mysqldump's C<--opt> argument, I<but not the same>.  It
does NOT pass C<--opt> to mysqldump.  Instead, it passes the following:

C<--add-drop-table> C<--add-locks> C<--allow-keywords> C<--comments>
C<--complete-insert> C<--create-options> C<--disable-keys>
C<--extended-insert> C<--quick> C<--quote-names> C<--set-charset>
C<--skip-lock-tables> C<--triggers> C<--tz-utc>

These are what I consider to be sensible default options.

=item --password

Password to use when connecting.

=item --port

Port number to use for connection.

=item --quiet

Suppresses the normal output (see L<"OUTPUT">).

=item --sets

Dump this comma-separated list of backup sets, in order.  Requires
L<"--settable">.  See L<"BACKUP SETS">.

=item --settable

Specifies the table in which backup sets are kept.  It may be given in
database.table form.

=item --socket

Socket file to use for connection.

=item --tab

Dump via C<SELECT INTO OUTFILE>, which is similar to what mysqldump does with the
C<--tab> option, but you're not constrained to a single database at a time.

Before you use this option, make sure you know what C<SELECT INTO OUTFILE> does!
I recommend only using it if you're running MySQL Parallel Dump on the same
machine as the MySQL server, but there is no protection if you don't.

The files will be gzipped after dumping if L<"--gzip"> is enabled.  This option
sets L<"--umask"> to zero so auto-created directories are writable by the MySQL
server.

=item --tables

Dump this comma-separated list of table (not database.table) names.  Only
applies when L<"--set"> is not given.

=item --tblregex

Dump only tables whose names match this Perl regular expression.  Only applies
when L<"--set"> is not given.

=item --test

Print commands instead of executing them.

=item --umask

Set the program's umask to this octal value.  This is useful when you want
created files and directories to be readable or writable by other users (for
example, the MySQL server itself).

=item --user

User for login if not current user.

=item --version

Output version information and exit.

=item --wait

If the MySQL server crashes during dumping, waits until the server comes back
and then continues with the rest of the tables.  The value is a number with a
suffix (s=seconds, m=minutes, h=hours, d=days).  MySQL Parallel Dump will
check the server every second until this time is exhausted, at which point it
will give up and exit.

This implements Peter Zaitsev's "safe dump" request: sometimes a dump on a
server that has corrupt data will kill the server, and this will just wait for
the server to restart, then keep going without retrying the table that killed
the server.  It's hard to say which table killed the server, so no tables will
be retried.  No additional locks will be taken after the server restarts; it's
assumed this behavior is only useful on a server you're not trying to dump
while it's in production.

=back

=head1 SYSTEM REQUIREMENTS

You need Perl, DBI, DBD::mysql, and some core packages that ought to be
installed in any reasonably new version of Perl.

=head1 BUGS

Please use the Sourceforge bug tracker, forums, and mailing lists to request
support or report bugs: L<http://sourceforge.net/projects/mysqltoolkit/>.

=head1 COPYRIGHT, LICENSE AND WARRANTY

This program is copyright (c) 2007 Baron Schwartz.  Feedback and improvements
are welcome.

THIS PROGRAM IS PROVIDED "AS IS" AND WITHOUT ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF
MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation, version 2; OR the Perl Artistic License.  On UNIX and similar
systems, you can issue `man perlgpl' or `man perlartistic' to read these
licenses.

You should have received a copy of the GNU General Public License along with
this program; if not, write to the Free Software Foundation, Inc., 59 Temple
Place, Suite 330, Boston, MA  02111-1307  USA.

=head1 AUTHOR

Baron Schwartz.

=head1 VERSION

This manual page documents Ver @VERSION@ Distrib @DISTRIB@ $Revision: 905 $.

=cut
